{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from Batch import nopeak_mask\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(SGDR=False, batchsize=1500, checkpoint=0, create_valset=False, d_model=200, device=0, dropout=0.1, epochs=2, floyd=False, heads=8, load_vocab='clir_it_en', load_weights='weights', lr=0.0001, max_strlen=80, n_layers=6, no_cuda=False, printevery=100, src_data='data/italian.txt', src_lang='it', trg_data='data/english.txt', trg_data_retrieval='data/LATIMESTEXT2.txt', trg_lang='en')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#If we are working on small dataset\n",
    "small = 0\n",
    "#If we want to activate relevance based training\n",
    "relevance_training = 1\n",
    "\n",
    "if small==1:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian_small.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english_small.txt')\n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/english_retrieval.txt')\n",
    "\n",
    "else:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english.txt')  \n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/LATIMESTEXT2.txt')\n",
    "\n",
    "parser.add_argument('-src_lang', type=str, default='it')\n",
    "parser.add_argument('-trg_lang', type=str, default='en')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=2)\n",
    "parser.add_argument('-d_model', type=int, default=200)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1500)\n",
    "parser.add_argument('-printevery', type=int, default=100)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights', type=str, default='weights')\n",
    "parser.add_argument('-load_vocab', type=str, default='clir_it_en')\n",
    "#parser.add_argument('-load_weights', type=str, default='tiny_train')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)\n",
    "\n",
    "#-load_weights clir_it_en -src_lang it -trg_lang en -k 2\n",
    "#opt = parser.parse_args()\n",
    "opt = parser.parse_args(args=[])\n",
    "opt.device = 0 if opt.no_cuda is False else -1\n",
    "print(opt)\n",
    "#assert opt.k > 0\n",
    "#assert opt.max_len > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(SGDR=False, batchsize=1500, checkpoint=0, create_valset=False, d_model=200, device=0, dropout=0.1, epochs=2, floyd=False, heads=8, load_vocab='clir_it_en', load_weights='weights', lr=0.0001, max_strlen=80, n_layers=6, no_cuda=False, printevery=100, src_data='data/italian.txt', src_lang='it', trg_data='data/english.txt', trg_data_retrieval='data/LATIMESTEXT2.txt', trg_lang='en')\n",
      "loading tokenizers...\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "        return text.split()\n",
    "\n",
    "def create_fields(opt):    \n",
    "    print(\"loading tokenizers...\") \n",
    "    TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=tokenizer)   \n",
    "    SRC = pickle.load(open(f'{opt.load_vocab}/SRC.pkl', 'rb'))\n",
    "    TRG = pickle.load(open(f'{opt.load_vocab}/TRG.pkl', 'rb'))\n",
    "    return(SRC, TRG)\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in open(opt.src_data)], 'trg': [line for line in open(opt.trg_data)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    print(\"creating target vocabulary ... \")\n",
    "    raw_data = {'trg': [line for line in open(opt.trg_data_retrieval)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"trg\"])\n",
    "    mask = (df['trg'].str.count(' ') > 1)\n",
    "    df = df.loc[mask]\n",
    "    df.to_csv(\"translate_transformer_retrieval_temp.csv\", index=False)\n",
    "    data_fields = [('trg', TRG)]\n",
    "    train_retrieval = data.TabularDataset('./translate_transformer_retrieval_temp.csv', format='csv', fields=data_fields)\n",
    "    os.remove('translate_transformer_retrieval_temp.csv')\n",
    "    SRC.build_vocab(train)\n",
    "    TRG.build_vocab(train, train_retrieval)\n",
    "    #TRG.build_vocab(train_temp)\n",
    "    if opt.checkpoint > 0:\n",
    "        try:\n",
    "            os.mkdir(\"weights\")\n",
    "        except:\n",
    "            print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
    "            quit()\n",
    "        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    return train_iter\n",
    "\n",
    "print(opt)\n",
    "SRC, TRG = create_fields(opt)\n",
    "#opt.train = create_dataset(opt, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained weights...\n"
     ]
    }
   ],
   "source": [
    "model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "opt.k=3\n",
    "opt.max_len=20\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "def preprocess_sentence(sentence, model, opt, SRC, TRG):\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "def translate_sentence(sentence, model, opt, SRC, TRG):\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    if opt.device == 0:\n",
    "        sentence = sentence.cuda()\n",
    "\n",
    "    sentences = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    print(sentences)\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        multiple_replace({' ?': '?', ' !': '!', ' .': '.', '\\' ': '\\'', ' ,': ','}, sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def translate(opt, model, SRC, TRG):\n",
    "    sentences = opt.text.lower().split('.')\n",
    "    translated = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for translated_sentence in translate_sentence(sentence + '.', model, opt, SRC, TRG):\n",
    "            translated.append(translated_sentence.capitalize())\n",
    "\n",
    "    return ('\\n'.join(translated))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #this is the output from the encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    #this is initializing the outputs \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    src_mask = src_mask.cuda()\n",
    "    trg_mask = trg_mask.cuda()\n",
    "    outputs = outputs.cuda()\n",
    "    e_output = e_output.cuda()\n",
    "\n",
    "    out = model.out(model.decoder(outputs,\n",
    "                                  e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    preds_token_ids = ix.view(ix.size(0), -1)\n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    print (pred_strings)\n",
    "    \n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2), e_output.size(-1))\n",
    "    if opt.device == 0:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    #print(\"indices of top k\")\n",
    "    #print(ix)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores\n",
    "\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, opt.max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "        src_mask = src_mask.cuda()\n",
    "        trg_mask = trg_mask.cuda()\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        # print(\"output data shape\")\n",
    "        # print(out.data.shape)\n",
    "\n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "\n",
    "        if (outputs == eos_tok).nonzero().size(0) == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1 / ((outputs == eos_tok).nonzero()[:, 1].type_as(log_scores) ** alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "\n",
    "    # if ind is None:\n",
    "    #     length = (outputs[0] == eos_tok).nonzero()[0]\n",
    "    #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    #\n",
    "    # else:\n",
    "    #     length = (outputs[ind] == eos_tok).nonzero()[0]\n",
    "    # return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "\n",
    "    if ind is None:\n",
    "        query_list = []\n",
    "        print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            length = (outputs[i] == eos_tok).nonzero()[0]\n",
    "            #length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list\n",
    "\n",
    "        # if (outputs[0]==eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "\n",
    "\n",
    "    else:\n",
    "        # if (outputs[ind] == eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "        query_list = []\n",
    "        print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            #length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt.text = \"epidemia ebola zaire recupera documenti parlano misure preventive prese dopo scoppio epidemia ebola zaire\"\n",
    "#opt.text = \"epidemia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of k is 3\n",
      "['outbreak west chad documents talk preventive measures taken outbreak', 'outbreak west bank documents talk preventive measures taken outbreak', 'outbreak west chad documents talk preventive measures taken outbreak outbreak']\n"
     ]
    }
   ],
   "source": [
    "phrase = translate(opt, model, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menu\n",
      "rise\n",
      "prove\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.itos[4880])\n",
    "print(TRG.vocab.itos[1283])\n",
    "print(TRG.vocab.itos[1977])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outbreak west chad documents talk preventive measures taken outbreak\n",
      "Outbreak west bank documents talk preventive measures taken outbreak\n",
      "Outbreak west chad documents talk preventive measures taken outbreak outbreak\n"
     ]
    }
   ],
   "source": [
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
