{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from Batch import nopeak_mask\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "os.chdir(\"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#If we are working on small dataset\n",
    "small = 0\n",
    "#If we want to activate relevance based training\n",
    "relevance_training = 1\n",
    "\n",
    "if small==1:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian_small.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english_small.txt')\n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/english_retrieval.txt')\n",
    "\n",
    "else:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english.txt')  \n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/LATIMESTEXT2.txt')\n",
    "\n",
    "parser.add_argument('-src_lang', type=str, default='it')\n",
    "parser.add_argument('-trg_lang', type=str, default='en')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=2)\n",
    "parser.add_argument('-d_model', type=int, default=200)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1500)\n",
    "parser.add_argument('-printevery', type=int, default=100)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights', type=str, default='weights')\n",
    "parser.add_argument('-load_vocab', type=str, default='clir_it_en')\n",
    "#parser.add_argument('-load_weights', type=str, default='tiny_train')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)\n",
    "\n",
    "#-load_weights clir_it_en -src_lang it -trg_lang en -k 2\n",
    "#opt = parser.parse_args()\n",
    "opt = parser.parse_args(args=[])\n",
    "opt.device = 0 if opt.no_cuda is False else -1\n",
    "#print(opt)\n",
    "#assert opt.k > 0\n",
    "#assert opt.max_len > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizers...\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "        return text.split()\n",
    "\n",
    "def create_fields(opt):    \n",
    "    print(\"loading tokenizers...\") \n",
    "    TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=tokenizer)   \n",
    "    SRC = pickle.load(open(f'{opt.load_vocab}/SRC.pkl', 'rb'))\n",
    "    TRG = pickle.load(open(f'{opt.load_vocab}/TRG.pkl', 'rb'))\n",
    "    return(SRC, TRG)\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in open(opt.src_data)], 'trg': [line for line in open(opt.trg_data)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    print(\"creating target vocabulary ... \")\n",
    "    raw_data = {'trg': [line for line in open(opt.trg_data_retrieval)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"trg\"])\n",
    "    mask = (df['trg'].str.count(' ') > 1)\n",
    "    df = df.loc[mask]\n",
    "    df.to_csv(\"translate_transformer_retrieval_temp.csv\", index=False)\n",
    "    data_fields = [('trg', TRG)]\n",
    "    train_retrieval = data.TabularDataset('./translate_transformer_retrieval_temp.csv', format='csv', fields=data_fields)\n",
    "    os.remove('translate_transformer_retrieval_temp.csv')\n",
    "    SRC.build_vocab(train)\n",
    "    TRG.build_vocab(train, train_retrieval)\n",
    "    #TRG.build_vocab(train_temp)\n",
    "    if opt.checkpoint > 0:\n",
    "        try:\n",
    "            os.mkdir(\"weights\")\n",
    "        except:\n",
    "            print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
    "            quit()\n",
    "        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    return train_iter\n",
    "\n",
    "#print(opt)\n",
    "SRC, TRG = create_fields(opt)\n",
    "#opt.train = create_dataset(opt, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import Transformer\n",
    "\n",
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_8'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_8_4'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_best_relevance'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_best_validation'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_1_epoch_large_full_relevance_new'))\n",
    "#model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "opt.k=3\n",
    "opt.max_len=20\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "def preprocess_sentence(sentence, model, opt, SRC, TRG):\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "def translate_sentence(sentence, model, opt, SRC, TRG):\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    if opt.device == 0:\n",
    "        sentence = sentence.cuda()\n",
    "\n",
    "    sentences, query, string_query = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    #print(sentences)\n",
    "    #print(query)\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        multiple_replace({' ?': '?', ' !': '!', ' .': '.', '\\' ': '\\'', ' ,': ','}, sentence)\n",
    "    return sentences, query, string_query\n",
    "\n",
    "\n",
    "def translate(opt, model, SRC, TRG):\n",
    "    sentences = opt.text.lower().split('.')\n",
    "    translated = []\n",
    "    queries = []\n",
    "    #print(translate_sentence(sentence + '.', model, opt, SRC, TRG))\n",
    "    for sentence in sentences:\n",
    "        translated_sentences, query, string_query = translate_sentence(sentence + '.', model, opt, SRC, TRG)\n",
    "        for translated_sentence in translated_sentences:\n",
    "            translated.append(translated_sentence.capitalize())\n",
    "        queries.append(query)\n",
    "\n",
    "    return (' '.join(translated)), queries, string_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #this is the output from the encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    #this is initializing the outputs \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    src_mask = src_mask.cuda()\n",
    "    trg_mask = trg_mask.cuda()\n",
    "    outputs = outputs.cuda()\n",
    "    e_output = e_output.cuda()\n",
    "\n",
    "    out = model.out(model.decoder(outputs,\n",
    "                                  e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    preds_token_ids = ix.view(ix.size(0), -1)\n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    \n",
    "    #print (pred_strings)\n",
    "    \n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2), e_output.size(-1))\n",
    "    if opt.device == 0:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    probs, ix = out[:, -1].data.topk(k)    \n",
    "    \n",
    "    preds_token_ids = ix.view(ix.size(0), -1)  #size = k * k \n",
    "    preds_probs = probs.view(probs.size(0), -1)\n",
    "    \n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    #print (pred_strings)\n",
    "    #pred_probs_string = [' '.join([str(ex[ind]) for ind in ex]) for ex in preds_probs]\n",
    "    pred_strings = []\n",
    "    pred_strings_dict = {}\n",
    "    for pred_token_id, prob in zip(preds_token_ids, preds_probs):\n",
    "        pred_strings_temp = ''\n",
    "        for iid, prob in zip(pred_token_id, prob):   \n",
    "            prob = prob.item()\n",
    "            if prob > 0.001: \n",
    "                pred_strings_temp+= str(TRG.vocab.itos[iid]) + ' '\n",
    "            if str(TRG.vocab.itos[iid]) in pred_strings_dict:\n",
    "                if prob > pred_strings_dict[str(TRG.vocab.itos[iid])]:\n",
    "                    pred_strings_dict[str(TRG.vocab.itos[iid])] = prob\n",
    "            else:\n",
    "                pred_strings_dict[str(TRG.vocab.itos[iid])] = prob\n",
    "                    \n",
    "        pred_strings.append(pred_strings_temp)\n",
    "    #print (preds_probs)\n",
    "    #print (pred_probs_strings)\n",
    "    \n",
    "    #print(\"indices of top k\")\n",
    "    #print(ix)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores, pred_strings, pred_strings_dict\n",
    "\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    query = {}\n",
    "    query_tokens = []\n",
    "    for i in range(2, opt.max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "        src_mask = src_mask.cuda()\n",
    "        trg_mask = trg_mask.cuda()\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))\n",
    "        #print (outputs.size())\n",
    "        #print (out.size())\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        # print(\"output data shape\")\n",
    "        # print(out.data.shape)\n",
    "\n",
    "        outputs, log_scores, pred_strings, pred_strings_dict = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "        \n",
    "#         This part is another way of forming the query dictionary \n",
    "        for pred_string in pred_strings: \n",
    "            pred_string_splitted = pred_string.split()\n",
    "            for st in pred_string_splitted:\n",
    "                query.setdefault(st, 1.0)\n",
    "                query[st] = query[st] + 1\n",
    "            query_tokens.extend(pred_string_splitted)\n",
    "        \n",
    "        for term in pred_strings_dict: \n",
    "            if term in query:\n",
    "                if pred_strings_dict[term] > query [term]:\n",
    "                    query[term] = pred_strings_dict[term]\n",
    "            else:\n",
    "                query[term] = pred_strings_dict[term]\n",
    "            \n",
    "        if (outputs == eos_tok).nonzero().size(0) == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1 / ((outputs == eos_tok).nonzero()[:, 1].type_as(log_scores) ** alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    #print(\"query\")\n",
    "    #print(query)\n",
    "    # if ind is None:\n",
    "    #     length = (outputs[0] == eos_tok).nonzero()[0]\n",
    "    #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    #\n",
    "    # else:\n",
    "    #     length = (outputs[ind] == eos_tok).nonzero()[0]\n",
    "    # return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "\n",
    "    if ind is None:\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            if eos_tok in outputs[i]:\n",
    "                length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            else:\n",
    "                length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query, query_tokens\n",
    "\n",
    "        # if (outputs[0]==eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "\n",
    "\n",
    "    else:\n",
    "        # if (outputs[ind] == eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            if eos_tok in outputs[i]:\n",
    "                length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            else:\n",
    "                length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query, query_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_galago_query(query_dict, text):\n",
    "    #print(\"printing query dict\")\n",
    "    #print(query_dict)\n",
    "    probs = [query_dict[key] for key in query_dict]\n",
    "    mean = statistics.mean(probs)\n",
    "    \n",
    "    text_splitted = text.split()\n",
    "    for token in text_splitted:\n",
    "        if token not in query_dict:\n",
    "              query_dict[token] = mean\n",
    "    probs = [query_dict[key] for key in query_dict]\n",
    "    sm = sum(probs)\n",
    "    new_query_dict = {} \n",
    "    for key in query_dict:\n",
    "        query_dict[key]/= sm\n",
    "        if(query_dict[key] > 0.1):\n",
    "            new_query_dict[key] = query_dict[key]\n",
    "    query_dict = new_query_dict\n",
    "    st=\"#combine:\"   \n",
    "    query_keys = query_dict.keys()\n",
    "    for index, val in enumerate(query_keys):\n",
    "        st+= str(index) + \"=\" + str(query_dict[val])\n",
    "        st+=\":\"\n",
    "    st = st.rstrip(\":\") \n",
    "    st+=\"(\"\n",
    "    for index, val in enumerate(query_keys):\n",
    "         st+= str(val + \" \")\n",
    "    st+=\")\"\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "opt.k = 6\n",
    "opt.text = \"epidemia ebola zaire recupera documenti parlano misure preventive prese dopo scoppio epidemia ebola zaire\"\n",
    "def load_query_tt(qdir, tt_file):\n",
    "    #passing query directory, query title and query title description\n",
    "    #Creating query_dict where title and query against a query id would be available\n",
    "    query_tt_dict = {} \n",
    "    query_file_tt = open(os.path.join(qdir, tt_file))\n",
    "    for line in query_file_tt:\n",
    "        if len(line) > 1: \n",
    "        #print(line)\n",
    "            line_splitted = line.split(\"\\t\")\n",
    "            query_title = line_splitted[1].strip()\n",
    "            query_translation_tt = line_splitted[2].strip()\n",
    "            query_tt_dict.setdefault(query_title, query_translation_tt)\n",
    "    return query_tt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel sentences\tweight neural\tMAP\n",
      "****************************************************\n",
      "query from dictionary\t5\t0.0\t0.2344\n",
      "query from dictionary\t5\t0.25\t0.2307\n",
      "query from dictionary\t5\t0.5\t0.23\n",
      "query from dictionary\t5\t0.75\t0.1723\n",
      "query from dictionary\t5\t1.0\t0.0549\n"
     ]
    }
   ],
   "source": [
    "def load_query_file(qdir, qtitle, qdesc):\n",
    "    #passing query directory, query title and query title description\n",
    "    #Creating query_dict where title and query against a query id would be available\n",
    "    query_file_title = open(os.path.join(qdir, qtitle))\n",
    "    query_dict = {}\n",
    "    query_title_len_dict = {}\n",
    "    for line in query_file_title:\n",
    "        line_splitted = line.split(\"\\t\")\n",
    "        query_id = line_splitted[0].strip()\n",
    "        query_title = line_splitted[1].strip()\n",
    "        query_dict.setdefault(query_id, [])\n",
    "        query_dict[query_id].append(query_title)\n",
    "        query_title_len_dict[query_id] = len(query_title.split())\n",
    "        #print(line)\n",
    "    query_file_desc = open(os.path.join(qdir, qdesc))\n",
    "    for line in query_file_desc:\n",
    "        line_splitted = line.split(\"\\t\")\n",
    "        query_id = line_splitted[0].strip()\n",
    "        query_title_desc = line_splitted[1].strip()\n",
    "        query_dict[query_id].append(query_title_desc)\n",
    "        title_len = query_title_len_dict[query_id]\n",
    "        query_desc = ' '.join(query_title_desc.split()[title_len: ])\n",
    "        query_dict[query_id].append(query_desc)\n",
    "    return query_dict\n",
    "\n",
    "\n",
    "#Query dict created\n",
    "#Order in which queries are put: title, title_description, description\n",
    "#load italian queries\n",
    "verbose = False\n",
    "single_test = True \n",
    "eng_query_dict = load_query_file(\"CLEF-ENG-ML/index/english/all\", \"All-eng-tit-final.tsv\", \"All-eng-tit-des-final-clef.tsv\")\n",
    "it_query_dict = load_query_file(\"CLEF-ENG-ML/index/italian\", \"All-Top-ita-tit-final.txt\", \"All-Top-ita-tit-desc-final.txt\")\n",
    "it_query_tt_dict = load_query_tt(\"CLEF-ENG-ML/index/italian\", \"italian_tt.txt\")\n",
    "print(\"parallel sentences\\tweight neural\\tMAP\")\n",
    "print(\"****************************************************\")\n",
    "\n",
    "if single_test == True:\n",
    "    params = {'k':[5], 'weight': np.linspace(0, 1, 5)}\n",
    "else:\n",
    "    params = {'k': range(5, 10), 'weight': np.linspace(0, 1, 5)}        \n",
    "    \n",
    "settings = {0:'query from dictionary'}\n",
    "for setting in settings.keys(): \n",
    "    for k in params['k']:\n",
    "        for weight in params['weight']:\n",
    "            opt.k = k \n",
    "            translation_file = open(os.path.join(\"CLEF-ENG-ML/index/english/all\", \"All-eng-tit-des-final.tsv\"), \"w\")\n",
    "            sum_percentage_uncovered = 0.0\n",
    "            list_of_references = []\n",
    "            hypotheses = []\n",
    "            length = 0\n",
    "            for key in sorted(it_query_dict)[0:10]:    \n",
    "                length+=1\n",
    "                #print(key)\n",
    "                opt.text = it_query_dict[key][1] ### getting the italian query \n",
    "                tt_query = it_query_tt_dict[opt.text]\n",
    "\n",
    "                if verbose == True:\n",
    "                    print(opt.text)\n",
    "                    print(\"translation table output\")\n",
    "                    print(tt_query)\n",
    "                candidate, queries, query_tokens = translate(opt, model, SRC, TRG)\n",
    "\n",
    "                if setting == 0:\n",
    "                    candidate = ' '.join(queries[0].keys()) ###\n",
    "                #else: \n",
    "                    #candidate = ' '.join(query_tokens)\n",
    "                #candidate+= ' '.join(queries[1].keys())\n",
    "                #candidate = candidate + \" \" + opt.text + \" \" + tt_query ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "                str_append = \" \"\n",
    "                for i in np.arange(opt.k):\n",
    "                    str_append+=opt.text + \" \"\n",
    "\n",
    "                #if setting==1:\n",
    "                #for i in np.arange(opt.k):\n",
    "                    #str_append+=tt_query + \" \"\n",
    "                candidate = candidate + \" \" + str_append  ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "                #candidate = tt_query\n",
    "                candidate = candidate.lower() ###\n",
    "                #candidate = create_galago_query(queries[0], opt.text + tt_query) ### weights estimated by neural approach. The code is put below\n",
    "                reference = eng_query_dict[key][1] ###\n",
    "                list_of_references.append(reference)\n",
    "                hypotheses.append(candidate)\n",
    "                reference_splitted = set(reference.split())\n",
    "                candidate_splitted = set(candidate.split())\n",
    "                uncovered = reference_splitted.difference(candidate_splitted)\n",
    "                if verbose== True:\n",
    "                    print(\"reference-------------------------------\")\n",
    "                    print(reference)\n",
    "                    print(\"candidate-------------------------------\")\n",
    "                    print(candidate)\n",
    "                    print(\"uncovered-------------------------------\")\n",
    "                    print(uncovered)\n",
    "                    print(\"uncovered%-------------------------------\")\n",
    "                percentage_uncovered = len(uncovered)/(len(reference_splitted) * 1.0)\n",
    "                sum_percentage_uncovered+=percentage_uncovered \n",
    "\n",
    "                if verbose == True:\n",
    "                    print(str(percentage_uncovered))\n",
    "                    print(\"******************************************\")\n",
    "\n",
    "                covered = reference_splitted.difference(uncovered)\n",
    "                str_uncovered = ' '.join(uncovered)\n",
    "                str_covered = ' '.join(covered)    \n",
    "                #translation_file.write(key + \"\\t\" + tt_query + \" \\n\")            \n",
    "                #translation_file.write(key + \"\\t\" + reference + \" \\n\")        \n",
    "                translation_file.write(key + \"\\t\" + \"#combine:0=\" + str(weight)+ \":1=\" + str(1-weight) + \"(#combine(\" + candidate + \") #combine(\" + tt_query + \"))\\n\")        \n",
    "                #translation_file.write(key + \"\\t\" + candidate + \" \" + reference + \" \\n\")    \n",
    "                #translation_file.write(key + \"\\t\" + \" \" + str_covered + \" \\n\")    \n",
    "                #translation_file.write(key + \"\\t\" + \" \" +  + \" \" + str_uncovered + \" \\n\")    \n",
    "\n",
    "            translation_file.close() \n",
    "            # Check current working directory.\n",
    "            # retval = os.getcwd()\n",
    "            os.chdir(\"CLEF-ENG-ML/\")\n",
    "            p = subprocess.Popen('/cm/shared/apps/java/jdk1.8.0_191/bin/java -cp target/lib/*:target/CLEF-ENG-ML-1.0-SNAPSHOT.jar rabflair.flair.myBatchSearch', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "            line = p.stdout.readlines()[0]\n",
    "            line_splitted = line.decode().split()\n",
    "            mAP = float(line_splitted[3])\n",
    "            #retval = p.wait()\n",
    "            os.chdir(\"../\")\n",
    "            if verbose == True:\n",
    "                print(\"percentage of uncovered terms \" + str(sum_percentage_uncovered/length))\n",
    "                #print(\"corpus bleu score is \" + str(corpus_bleu(list_of_references, hypotheses)) + \" and average MAP is \" + str(mAP))\n",
    "                #print(\"BLEU:\\t\" + str(corpus_bleu(list_of_references, hypotheses)))\n",
    "\n",
    "            print(settings[setting] + \"\\t\" + str(opt.k) + \"\\t\" + str(weight) + \"\\t\" + str(mAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#os.chdir(\"../\")\n",
    "#print(os.getcwd())\n",
    "# for line in query_file_desc:\n",
    "#     print(line)\n",
    "# print(query_file_desc)\n",
    "\n",
    "# def create_galago_query(tuple_list):\n",
    "#     st=\"#combine:\"\n",
    "    \n",
    "#     for index, tuple in enumerate(tuple_list):\n",
    "#         st+= str(index) + \"=\" + str(tuple[1])\n",
    "#         st+=\":\"\n",
    "#     st = st.rstrip(\":\") \n",
    "#     st+=\"(\"\n",
    "#     for index, tuple in enumerate(tuple_list):\n",
    "#          st+= str(tuple[0] + \" \")\n",
    "#     st+=\")\"\n",
    "#     print(st)\n",
    "    \n",
    "# import pandas as pd\n",
    "# import operator\n",
    "# for query in queries:\n",
    "#     sum = 0.0\n",
    "#     for token in query.keys():\n",
    "#         sum+=query[token]\n",
    "#     for token in query.keys():\n",
    "#         query[token]/=sum\n",
    "#     sorted_query = sorted(query.items(), key=operator.itemgetter(1))\n",
    "#     print (sorted_query)\n",
    "#     create_galago_query(sorted_query)\n",
    "#     #print(sorted(queries, lambda key = queries[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tagme \n",
    "# import string\n",
    "# print(type(TRG))\n",
    "# TRG.vocab.stoi['balladur']\n",
    "# tagme.GCUBE_TOKEN = \"58f5b784-bf5a-4840-a928-e214134f98e7-843339462\"\n",
    "# regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# def get_entity_strings(query, language):\n",
    "#     annotations = tagme.annotate(query, lang=language)\n",
    "#     st = ''\n",
    "#     for ann in annotations.get_annotations(0.3):\n",
    "#         #print ann.entity_title\n",
    "#         entity = ann.entity_title.lower()\n",
    "#         st+= entity + ' '\n",
    "#     return st\n",
    "# print(get_entity_strings(\"palmares ayrton senna\", 'it'))\n",
    "#     #print(st)\n",
    "# #opt.text = \"german parlamento documenti artista christo\"\n",
    "# #opt.text = \"trova documenti parlano impacchettamento parlamento tedesco berlino opera artista christo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     length+=1\n",
    "#     #print(key)\n",
    "#     opt.text = it_query_dict[key][1] ###\n",
    "#     print(opt.text)\n",
    "#     candidate, queries = translate(opt, model, SRC, TRG)\n",
    "#     #candidate = ' '.join(queries[0].keys()) ###\n",
    "#     #candidate+= ' '.join(queries[1].keys())\n",
    "#     candidate = candidate + \" \" + opt.text ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "#     candidate = candidate.lower() ###\n",
    "#     #candidate = create_galago_query(queries[0], opt.text) ### weights estimated by neural approach. The code is put below\n",
    "#     reference = eng_query_dict[key][1] ###\n",
    "#     list_of_references.append(reference)\n",
    "#     hypotheses.append(candidate)\n",
    "#     reference_splitted_list = reference.split() \n",
    "#     reference_splitted_list = [stemmer.stem(token) for token in reference_splitted_list]    \n",
    "#     reference_splitted = set(reference_splitted_list)\n",
    "#     candidate_splitted_list = candidate.split() \n",
    "#     candidate_splitted_list = [stemmer.stem(token)  for token in candidate_splitted_list]\n",
    "#     candidate_splitted = set(candidate_splitted_list)\n",
    "#     #reference = ' '.join(reference_splitted)\n",
    "#     #candidate = ' '.join(candidate_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
