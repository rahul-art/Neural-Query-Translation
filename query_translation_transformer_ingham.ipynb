{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from Batch import nopeak_mask\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(SGDR=False, batchsize=1500, checkpoint=0, create_valset=False, d_model=200, device=0, dropout=0.1, epochs=2, floyd=False, heads=8, load_vocab='clir_it_en', load_weights='weights', lr=0.0001, max_strlen=80, n_layers=6, no_cuda=False, printevery=100, src_data='data/italian.txt', src_lang='it', trg_data='data/english.txt', trg_data_retrieval='data/LATIMESTEXT2.txt', trg_lang='en')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#If we are working on small dataset\n",
    "small = 0\n",
    "#If we want to activate relevance based training\n",
    "relevance_training = 1\n",
    "\n",
    "if small==1:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian_small.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english_small.txt')\n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/english_retrieval.txt')\n",
    "\n",
    "else:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english.txt')  \n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/LATIMESTEXT2.txt')\n",
    "\n",
    "parser.add_argument('-src_lang', type=str, default='it')\n",
    "parser.add_argument('-trg_lang', type=str, default='en')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=2)\n",
    "parser.add_argument('-d_model', type=int, default=200)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1500)\n",
    "parser.add_argument('-printevery', type=int, default=100)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights', type=str, default='weights')\n",
    "parser.add_argument('-load_vocab', type=str, default='clir_it_en')\n",
    "#parser.add_argument('-load_weights', type=str, default='tiny_train')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)\n",
    "\n",
    "#-load_weights clir_it_en -src_lang it -trg_lang en -k 2\n",
    "#opt = parser.parse_args()\n",
    "opt = parser.parse_args(args=[])\n",
    "opt.device = 0 if opt.no_cuda is False else -1\n",
    "print(opt)\n",
    "#assert opt.k > 0\n",
    "#assert opt.max_len > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(SGDR=False, batchsize=1500, checkpoint=0, create_valset=False, d_model=200, device=0, dropout=0.1, epochs=2, floyd=False, heads=8, load_vocab='clir_it_en', load_weights='weights', lr=0.0001, max_strlen=80, n_layers=6, no_cuda=False, printevery=100, src_data='data/italian.txt', src_lang='it', trg_data='data/english.txt', trg_data_retrieval='data/LATIMESTEXT2.txt', trg_lang='en')\n",
      "loading tokenizers...\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "        return text.split()\n",
    "\n",
    "def create_fields(opt):    \n",
    "    print(\"loading tokenizers...\") \n",
    "    TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=tokenizer)   \n",
    "    SRC = pickle.load(open(f'{opt.load_vocab}/SRC.pkl', 'rb'))\n",
    "    TRG = pickle.load(open(f'{opt.load_vocab}/TRG.pkl', 'rb'))\n",
    "    return(SRC, TRG)\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in open(opt.src_data)], 'trg': [line for line in open(opt.trg_data)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    print(\"creating target vocabulary ... \")\n",
    "    raw_data = {'trg': [line for line in open(opt.trg_data_retrieval)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"trg\"])\n",
    "    mask = (df['trg'].str.count(' ') > 1)\n",
    "    df = df.loc[mask]\n",
    "    df.to_csv(\"translate_transformer_retrieval_temp.csv\", index=False)\n",
    "    data_fields = [('trg', TRG)]\n",
    "    train_retrieval = data.TabularDataset('./translate_transformer_retrieval_temp.csv', format='csv', fields=data_fields)\n",
    "    os.remove('translate_transformer_retrieval_temp.csv')\n",
    "    SRC.build_vocab(train)\n",
    "    TRG.build_vocab(train, train_retrieval)\n",
    "    #TRG.build_vocab(train_temp)\n",
    "    if opt.checkpoint > 0:\n",
    "        try:\n",
    "            os.mkdir(\"weights\")\n",
    "        except:\n",
    "            print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
    "            quit()\n",
    "        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    return train_iter\n",
    "\n",
    "print(opt)\n",
    "SRC, TRG = create_fields(opt)\n",
    "#opt.train = create_dataset(opt, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Models import Transformer\n",
    "\n",
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_1_epoch_large'))\n",
    "#model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "opt.k=3\n",
    "opt.max_len=20\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "def preprocess_sentence(sentence, model, opt, SRC, TRG):\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "def translate_sentence(sentence, model, opt, SRC, TRG):\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    if opt.device == 0:\n",
    "        sentence = sentence.cuda()\n",
    "\n",
    "    sentences, query = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    #print(sentences)\n",
    "    #print(query)\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        multiple_replace({' ?': '?', ' !': '!', ' .': '.', '\\' ': '\\'', ' ,': ','}, sentence)\n",
    "    return sentences, query\n",
    "\n",
    "\n",
    "def translate(opt, model, SRC, TRG):\n",
    "    sentences = opt.text.lower().split('.')\n",
    "    translated = []\n",
    "    queries = []\n",
    "    #print(translate_sentence(sentence + '.', model, opt, SRC, TRG))\n",
    "    for sentence in sentences:\n",
    "        translated_sentences, query = translate_sentence(sentence + '.', model, opt, SRC, TRG)\n",
    "        for translated_sentence in translated_sentences:\n",
    "            translated.append(translated_sentence.capitalize())\n",
    "        queries.append(query)\n",
    "\n",
    "    return ('\\n'.join(translated)), queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #this is the output from the encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    #this is initializing the outputs \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    src_mask = src_mask.cuda()\n",
    "    trg_mask = trg_mask.cuda()\n",
    "    outputs = outputs.cuda()\n",
    "    e_output = e_output.cuda()\n",
    "\n",
    "    out = model.out(model.decoder(outputs,\n",
    "                                  e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    preds_token_ids = ix.view(ix.size(0), -1)\n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    print (pred_strings)\n",
    "    \n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2), e_output.size(-1))\n",
    "    if opt.device == 0:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    preds_token_ids = ix.view(ix.size(0), -1)\n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    print (pred_strings)\n",
    "    \n",
    "    #print(\"indices of top k\")\n",
    "    #print(ix)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores, pred_strings\n",
    "\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    query = {}\n",
    "    for i in range(2, opt.max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "        src_mask = src_mask.cuda()\n",
    "        trg_mask = trg_mask.cuda()\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        # print(\"output data shape\")\n",
    "        # print(out.data.shape)\n",
    "\n",
    "        outputs, log_scores, pred_strings = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "        \n",
    "        for pred_string in pred_strings: \n",
    "            pred_string_splitted = pred_string.split()\n",
    "            for st in pred_string_splitted:\n",
    "                query.setdefault(st, 1.0)\n",
    "                query[st] = query[st] + 1\n",
    "        \n",
    "        if (outputs == eos_tok).nonzero().size(0) == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1 / ((outputs == eos_tok).nonzero()[:, 1].type_as(log_scores) ** alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    #print(\"query\")\n",
    "    #print(query)\n",
    "    # if ind is None:\n",
    "    #     length = (outputs[0] == eos_tok).nonzero()[0]\n",
    "    #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    #\n",
    "    # else:\n",
    "    #     length = (outputs[ind] == eos_tok).nonzero()[0]\n",
    "    # return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "\n",
    "    if ind is None:\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            length = (outputs[i] == eos_tok).nonzero()[0]\n",
    "            #length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query\n",
    "\n",
    "        # if (outputs[0]==eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "\n",
    "\n",
    "    else:\n",
    "        # if (outputs[ind] == eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            #length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt.text = \"epidemia ebola zaire recupera documenti parlano misure preventive prese dopo scoppio epidemia ebola zaire\"\n",
    "#opt.text = \"german parlamento documenti artista christo\"\n",
    "#opt.text = \"trova documenti parlano impacchettamento parlamento tedesco berlino opera artista christo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epidemic outbreak famine']\n",
      "['zaire famine devastated', 'zaire mozambique saudi', 'zaire devastated mozambique']\n",
      "['recover returning getting', 'returning recover will', 'zaire mozambique saudi']\n",
      "['will get getting', 'documents papers reports', 'documents papers reports']\n",
      "['talk speak talking', 'talk talking speak', 'get acquire pass']\n",
      "['preventive prevention measures', 'preventive prevention measures', 'preventive prevention measures']\n",
      "['measures action steps', 'measures action steps', 'measures action steps']\n",
      "['taken took adopted', 'taken took outbreak', 'taken took adopted']\n",
      "['outbreak epidemic disease', 'outbreak epidemic disease', 'outbreak epidemic disease']\n",
      "['epidemic outbreak disease', 'epidemic outbreak disease', 'epidemic outbreak disease']\n",
      "['caused epidemic outbreak', 'caused epidemic outbreak', 'epidemic outbreak disease']\n",
      "['<eos> sherlock delaware', '<eos> disease epidemic', '<eos> caused devastated']\n",
      "['<eos> disease weapons', '<eos> weapons base', '<eos> elaborating sherlock']\n",
      "['<eos> disease weapons', '<eos> weapons disease', '<eos> weapons base']\n",
      "['<eos> disease weapons', '<eos> will documents', '<eos> weapons will']\n",
      "['<eos> disease will', '<eos> documents will', '<eos> will documents']\n",
      "['<eos> documents disease', '<eos> documents will', '<eos> documents will']\n",
      "['<eos> documents disease', '<eos> documents will', '<eos> will can']\n",
      "['<eos> documents disease', '<eos> documents document', '<eos> will can']\n",
      "[('famine', 0.01), ('acquire', 0.01), ('pass', 0.01), ('delaware', 0.01), ('elaborating', 0.01), ('document', 0.01), ('saudi', 0.015), ('recover', 0.015), ('returning', 0.015), ('getting', 0.015), ('get', 0.015), ('papers', 0.015), ('reports', 0.015), ('talk', 0.015), ('speak', 0.015), ('talking', 0.015), ('adopted', 0.015), ('sherlock', 0.015), ('base', 0.015), ('can', 0.015), ('devastated', 0.02), ('mozambique', 0.02), ('preventive', 0.02), ('prevention', 0.02), ('action', 0.02), ('steps', 0.02), ('taken', 0.02), ('took', 0.02), ('caused', 0.02), ('zaire', 0.025), ('measures', 0.035), ('weapons', 0.04), ('outbreak', 0.055), ('epidemic', 0.055), ('will', 0.065), ('documents', 0.065), ('disease', 0.085), ('<eos>', 0.125)]\n",
      "#combine:0=0.01:1=0.01:2=0.01:3=0.01:4=0.01:5=0.01:6=0.015:7=0.015:8=0.015:9=0.015:10=0.015:11=0.015:12=0.015:13=0.015:14=0.015:15=0.015:16=0.015:17=0.015:18=0.015:19=0.015:20=0.02:21=0.02:22=0.02:23=0.02:24=0.02:25=0.02:26=0.02:27=0.02:28=0.02:29=0.025:30=0.035:31=0.04:32=0.055:33=0.055:34=0.065:35=0.065:36=0.085:37=0.125(famine acquire pass delaware elaborating document saudi recover returning getting get papers reports talk speak talking adopted sherlock base can devastated mozambique preventive prevention action steps taken took caused zaire measures weapons outbreak epidemic will documents disease <eos> )\n"
     ]
    }
   ],
   "source": [
    "phrase, queries = translate(opt, model, SRC, TRG)\n",
    "\n",
    "\n",
    "def create_galago_query(tuple_list):\n",
    "    st=\"#combine:\"\n",
    "    \n",
    "    for index, tuple in enumerate(tuple_list):\n",
    "        st+= str(index) + \"=\" + str(tuple[1])\n",
    "        st+=\":\"\n",
    "    st = st.rstrip(\":\") \n",
    "    st+=\"(\"\n",
    "    for index, tuple in enumerate(tuple_list):\n",
    "         st+= str(tuple[0] + \" \")\n",
    "    st+=\")\"\n",
    "    print(st)\n",
    "    \n",
    "import pandas as pd\n",
    "import operator\n",
    "for query in queries:\n",
    "    sum = 0.0\n",
    "    for token in query.keys():\n",
    "        sum+=query[token]\n",
    "    for token in query.keys():\n",
    "        query[token]/=sum\n",
    "    sorted_query = sorted(query.items(), key=operator.itemgetter(1))\n",
    "    print (sorted_query)\n",
    "    create_galago_query(sorted_query)\n",
    "    #print(sorted(queries, lambda key = queries[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menu\n",
      "rise\n",
      "prove\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.itos[4880])\n",
    "print(TRG.vocab.itos[1283])\n",
    "print(TRG.vocab.itos[1977])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outbreak zaire recover documents talk preventive measures taken outbreak outbreak outbreak\n",
      "Outbreak zaire recover documents talk preventive measures taken outbreak outbreak epidemic\n",
      "Outbreak zaire recover documents talk preventive measures taken outbreak outbreak outbreak\n"
     ]
    }
   ],
   "source": [
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
