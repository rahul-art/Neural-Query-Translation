{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from Batch import nopeak_mask\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "os.chdir(\"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#If we are working on small dataset\n",
    "small = 0\n",
    "#If we want to activate relevance based training\n",
    "relevance_training = 1\n",
    "\n",
    "if small==1:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian_small.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english_small.txt')\n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/english_retrieval.txt')\n",
    "\n",
    "else:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english.txt')  \n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/LATIMESTEXT2.txt')\n",
    "\n",
    "parser.add_argument('-src_lang', type=str, default='it')\n",
    "parser.add_argument('-trg_lang', type=str, default='en')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=2)\n",
    "parser.add_argument('-d_model', type=int, default=200)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1500)\n",
    "parser.add_argument('-printevery', type=int, default=100)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights', type=str, default='weights')\n",
    "parser.add_argument('-load_vocab', type=str, default='clir_it_en')\n",
    "#parser.add_argument('-load_weights', type=str, default='tiny_train')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)\n",
    "\n",
    "#-load_weights clir_it_en -src_lang it -trg_lang en -k 2\n",
    "#opt = parser.parse_args()\n",
    "opt = parser.parse_args(args=[])\n",
    "opt.device = 0 if opt.no_cuda is False else -1\n",
    "#print(opt)\n",
    "#assert opt.k > 0\n",
    "#assert opt.max_len > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizers...\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "        return text.split()\n",
    "\n",
    "def create_fields(opt):    \n",
    "    print(\"loading tokenizers...\") \n",
    "    TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=tokenizer)   \n",
    "    SRC = pickle.load(open(f'{opt.load_vocab}/SRC.pkl', 'rb'))\n",
    "    TRG = pickle.load(open(f'{opt.load_vocab}/TRG.pkl', 'rb'))\n",
    "    return(SRC, TRG)\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in open(opt.src_data)], 'trg': [line for line in open(opt.trg_data)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    print(\"creating target vocabulary ... \")\n",
    "    raw_data = {'trg': [line for line in open(opt.trg_data_retrieval)]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"trg\"])\n",
    "    mask = (df['trg'].str.count(' ') > 1)\n",
    "    df = df.loc[mask]\n",
    "    df.to_csv(\"translate_transformer_retrieval_temp.csv\", index=False)\n",
    "    data_fields = [('trg', TRG)]\n",
    "    train_retrieval = data.TabularDataset('./translate_transformer_retrieval_temp.csv', format='csv', fields=data_fields)\n",
    "    os.remove('translate_transformer_retrieval_temp.csv')\n",
    "    SRC.build_vocab(train)\n",
    "    TRG.build_vocab(train, train_retrieval)\n",
    "    #TRG.build_vocab(train_temp)\n",
    "    if opt.checkpoint > 0:\n",
    "        try:\n",
    "            os.mkdir(\"weights\")\n",
    "        except:\n",
    "            print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
    "            quit()\n",
    "        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    return train_iter\n",
    "\n",
    "#print(opt)\n",
    "SRC, TRG = create_fields(opt)\n",
    "#opt.train = create_dataset(opt, SRC, TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Models import Transformer\n",
    "CONTEXT_SIZE = 2\n",
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt.d_model, opt.n_layers, opt.heads, opt.dropout, CONTEXT_SIZE)\n",
    "model = model.cuda()\n",
    "#model_name = \"so_far_best_validation\"\n",
    "#model_name = \"model_weights_8_1\"\n",
    "model_name = \"model_weights_best_validation\"\n",
    "#model_name = \"model_weights_best_relevance\"\n",
    "#model_name = \"model_weights_1_epoch_large_full_relevance_new\"\n",
    "#model_name = \"model_weights\"\n",
    "\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_8'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_8_4'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/' + model_name ))\n",
    "model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_best_validation'))\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/so_far_best_validation_new'))\n",
    "\n",
    "#model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights_1_epoch_large_full_relevance_new'))\n",
    "#model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "opt.k=3\n",
    "opt.max_len=20\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "def preprocess_sentence(sentence, model, opt, SRC, TRG):\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "def translate_sentence(sentence, model, opt, SRC, TRG):\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != 0 or opt.floyd == True:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    if opt.device == 0:\n",
    "        sentence = sentence.cuda()\n",
    "\n",
    "    sentences, query, string_query = beam_search(sentence, model, SRC, TRG, opt)\n",
    "    #print(sentences)\n",
    "    #print(query)\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        multiple_replace({' ?': '?', ' !': '!', ' .': '.', '\\' ': '\\'', ' ,': ','}, sentence)\n",
    "    return sentences, query, string_query\n",
    "\n",
    "\n",
    "def translate(opt, model, SRC, TRG):\n",
    "    sentences = opt.text.lower().split('.')\n",
    "    translated = []\n",
    "    queries = []\n",
    "    #print(translate_sentence(sentence + '.', model, opt, SRC, TRG))\n",
    "    for sentence in sentences:\n",
    "        translated_sentences, query, string_query = translate_sentence(sentence + '.', model, opt, SRC, TRG)\n",
    "        for translated_sentence in translated_sentences:\n",
    "            translated.append(translated_sentence.capitalize())\n",
    "        queries.append(query)\n",
    "\n",
    "    return (' '.join(translated)), queries, string_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    #this is the output from the encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    #this is initializing the outputs \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    src_mask = src_mask.cuda()\n",
    "    trg_mask = trg_mask.cuda()\n",
    "    outputs = outputs.cuda()\n",
    "    e_output = e_output.cuda()\n",
    "\n",
    "    out = model.out(model.decoder(outputs,\n",
    "                                  e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    preds_token_ids = ix.view(ix.size(0), -1)\n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    \n",
    "    #print (pred_strings)\n",
    "    \n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2), e_output.size(-1))\n",
    "    if opt.device == 0:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    probs, ix = out[:, -1].data.topk(k)    \n",
    "    \n",
    "    preds_token_ids = ix.view(ix.size(0), -1)  #size = k * k \n",
    "    preds_probs = probs.view(probs.size(0), -1)\n",
    "    \n",
    "    pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "    print (pred_strings)\n",
    "    #pred_probs_string = [' '.join([str(ex[ind]) for ind in ex]) for ex in preds_probs]\n",
    "    pred_strings = []\n",
    "    pred_strings_dict = {}\n",
    "    for pred_token_id, prob in zip(preds_token_ids, preds_probs):\n",
    "        pred_strings_temp = ''\n",
    "        for iid, prob in zip(pred_token_id, prob):   \n",
    "            prob = prob.item()\n",
    "            if prob > 0.001: \n",
    "                pred_strings_temp+= str(TRG.vocab.itos[iid]) + ' '\n",
    "            if str(TRG.vocab.itos[iid]) in pred_strings_dict:\n",
    "                if prob > pred_strings_dict[str(TRG.vocab.itos[iid])]:\n",
    "                    pred_strings_dict[str(TRG.vocab.itos[iid])] = prob\n",
    "            else:\n",
    "                pred_strings_dict[str(TRG.vocab.itos[iid])] = prob\n",
    "                    \n",
    "        pred_strings.append(pred_strings_temp)\n",
    "    #print (preds_probs)\n",
    "    #print (pred_probs_strings)\n",
    "    \n",
    "    #print(\"indices of top k\")\n",
    "    #print(ix)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores, pred_strings, pred_strings_dict\n",
    "\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    query = {}\n",
    "    query_tokens = []\n",
    "    for i in range(2, opt.max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "        src_mask = src_mask.cuda()\n",
    "        trg_mask = trg_mask.cuda()\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))\n",
    "        #print (outputs.size())\n",
    "        #print (out.size())\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        # print(\"output data shape\")\n",
    "        # print(out.data.shape)\n",
    "\n",
    "        outputs, log_scores, pred_strings, pred_strings_dict = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "        \n",
    "#         This part is another way of forming the query dictionary \n",
    "        for pred_string in pred_strings: \n",
    "            pred_string_splitted = pred_string.split()\n",
    "            for st in pred_string_splitted:\n",
    "                query.setdefault(st, 1.0)\n",
    "                query[st] = query[st] + 1\n",
    "            query_tokens.extend(pred_string_splitted)\n",
    "        \n",
    "        for term in pred_strings_dict: \n",
    "            if term in query:\n",
    "                if pred_strings_dict[term] > query [term]:\n",
    "                    query[term] = pred_strings_dict[term]\n",
    "            else:\n",
    "                query[term] = pred_strings_dict[term]\n",
    "            \n",
    "        if (outputs == eos_tok).nonzero().size(0) == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1 / ((outputs == eos_tok).nonzero()[:, 1].type_as(log_scores) ** alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    #print(\"query\")\n",
    "    #print(query)\n",
    "    # if ind is None:\n",
    "    #     length = (outputs[0] == eos_tok).nonzero()[0]\n",
    "    #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    #\n",
    "    # else:\n",
    "    #     length = (outputs[ind] == eos_tok).nonzero()[0]\n",
    "    # return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "\n",
    "    if ind is None:\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            if eos_tok in outputs[i]:\n",
    "                length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            else:\n",
    "                length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query, query_tokens\n",
    "\n",
    "        # if (outputs[0]==eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "\n",
    "\n",
    "    else:\n",
    "        # if (outputs[ind] == eos_tok).nonzero().size(0) >= 1:\n",
    "        #     length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        #     return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])\n",
    "        # else:\n",
    "        #     return ' '\n",
    "        query_list = []\n",
    "        #print(\"value of k is \" + str(opt.k))\n",
    "        for i in np.arange(opt.k):\n",
    "            if eos_tok in outputs[i]:\n",
    "                length = (outputs[i]==eos_tok).nonzero()[0]\n",
    "            else:\n",
    "                length = opt.max_len\n",
    "            query_list.append(' '.join([TRG.vocab.itos[tok] for tok in outputs[i][1:length]]))\n",
    "        return query_list, query, query_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.field.Field'>\n"
     ]
    }
   ],
   "source": [
    "print(type(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_galago_query(query_dict, text):\n",
    "    print(\"printing query dict\")\n",
    "    print(query_dict)\n",
    "    probs = [query_dict[key] for key in query_dict]\n",
    "    mean = statistics.mean(probs)\n",
    "    \n",
    "    text_splitted = text.split()\n",
    "    for token in text_splitted:\n",
    "        if token not in query_dict:\n",
    "              query_dict[token] = mean\n",
    "    probs = [query_dict[key] for key in query_dict]\n",
    "    sm = sum(probs)\n",
    "    new_query_dict = {} \n",
    "    for key in query_dict:\n",
    "        query_dict[key]/= sm\n",
    "        if(query_dict[key] > 0.1):\n",
    "            new_query_dict[key] = query_dict[key]\n",
    "    query_dict = new_query_dict\n",
    "    st=\"#combine:\"   \n",
    "    query_keys = query_dict.keys()\n",
    "    for index, val in enumerate(query_keys):\n",
    "        st+= str(index) + \"=\" + str(query_dict[val])\n",
    "        st+=\":\"\n",
    "    st = st.rstrip(\":\") \n",
    "    st+=\"(\"\n",
    "    for index, val in enumerate(query_keys):\n",
    "         st+= str(val + \" \")\n",
    "    st+=\")\"\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "opt.k = 6\n",
    "opt.text = \"epidemia ebola zaire recupera documenti parlano misure preventive prese dopo scoppio epidemia ebola zaire\"\n",
    "def load_query_tt(qdir, tt_file):\n",
    "    #passing query directory, query title and query title description\n",
    "    #Creating query_dict where title and query against a query id would be available\n",
    "    query_tt_dict = {} \n",
    "    query_file_tt = open(os.path.join(qdir, tt_file))\n",
    "    for line in query_file_tt:\n",
    "        if len(line) > 1: \n",
    "        #print(line)\n",
    "            line_splitted = line.split(\"\\t\")\n",
    "            query_title = line_splitted[1].strip()\n",
    "            query_translation_tt = line_splitted[2].strip()\n",
    "            query_tt_dict.setdefault(query_title, query_translation_tt)\n",
    "    return query_tt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_weights_best_validation\n",
      "parallel sentences\tweight neural\tMAP\n",
      "****************************************************\n",
      "architettura berlino trova documenti riguardano architettura berlino\n",
      "architettura berlino trova documenti riguardano architettura berlino\n",
      "translation table output\n",
      "#combine:0=0.14:1=0.14:2=0.14:3=0.14:4=0.14:5=0.14:6=0.14( #combine:0=0.77:1=0.10( architecture structure )#combine:0=0.96:1=0.00( berlin summit )#combine:0=0.17:1=0.09( finds find )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.20:1=0.12( concern relate )#combine:0=0.77:1=0.10( architecture structure )#combine:0=0.96:1=0.00( berlin summit ))\n",
      "['architecture structure framework philosophy infrastructure', 'berlin found came reached set', 'architecture structure framework philosophy berlin', 'berlin documents nice charter white', 'berlin found galileo set architecture']\n",
      "['found documents document find paper', 'found documents document paper find', 'found documents document paper find', 'found document find paper documents', 'found documents paper document find']\n",
      "['documents document paper papers texts', 'concern relate relating affecting concerning', 'concern relate concerns relates affecting', 'documents document paper papers texts', 'concern relate concerns affecting relates']\n",
      "['concern relate relating affecting concerning', 'architecture galileo eurostat foundations structure', 'concern relate affecting relating concerning', 'architecture galileo foundations eurostat infrastructures', 'concern relate affecting concerning relating']\n",
      "['architecture galileo foundations eurostat infrastructures', 'architecture galileo foundations eurostat infrastructures', '<eos> speedy envisaged quicker authorisation', 'architecture galileo foundations infrastructures eurostat', 'architecture galileo foundations sme eurostat']\n",
      "['<eos> speedy envisaged quicker introductory', 'envisaged authorisation speedy <eos> introductory', 'architecture <eos> galileo logistics profitability', 'envisaged <eos> speedy authorisation introductory', 'envisaged speedy <eos> authorisation quicker']\n",
      "['presupposes oblige accede sv generalised', '<eos> circulation change allowance period', '<eos> european charter parliament envisaged', '<eos> circulation people expression charter', 'paper remarks text statement statements']\n",
      "['presupposes oblige accede railway sv', 'presupposes oblige accede sv railway', '<eos> european charter berlin house', 'presupposes oblige accede sv railway', '<eos> within pensions public european']\n",
      "reference-------------------------------\n",
      "architecture berlin find documents architecture berlin\n",
      "candidate-------------------------------\n",
      "berlin architecture found documents concern architecture introductory paper berlin architecture found documents concern architecture speedy circulation berlin architecture found documents concern architecture envisaged berlin architecture found documents concern architecture envisaged berlin architecture found documents concern architecture quicker  \n",
      "uncovered-------------------------------\n",
      "{'find'}\n",
      "uncovered%-------------------------------\n",
      "0.25\n",
      "******************************************\n",
      "guerre radio ruolo radio durante guerre conflitti armati\n",
      "guerre radio ruolo radio durante guerre conflitti armati\n",
      "translation table output\n",
      "#combine:0=0.12:1=0.12:2=0.12:3=0.12:4=0.12:5=0.12:6=0.12:7=0.12( #combine:0=0.74:1=0.19( wars war )#combine:0=0.79:1=0.05( radio broadcasting )#combine:0=0.86:1=0.05( role part )#combine:0=0.79:1=0.05( radio broadcasting )#combine:0=0.28:1=0.23( throughout course )#combine:0=0.74:1=0.19( wars war )#combine:0=0.59:1=0.31( conflicts conflict )#combine:0=0.90:1=0.02( armed military ))\n",
      "['radio broadcasting broadcasters satellite concentration', 'wars war conflicts conflict dictatorships', 'radio broadcasting satellite broadcasters concentration', 'radio broadcasting satellite including broadcasters', 'radio including war known like']\n",
      "['broadcasting radio stations television power', 'role radio play broadcasting playing', 'broadcasting radio television stations role', 'role play radio playing plays', 'broadcasting role radio play roles']\n",
      "['role play plays playing also', 'radio broadcasting satellite television culture', 'role play broadcasting radio playing', 'role play plays radio roles', 'role broadcasting radio play playing']\n",
      "['war conflicts wars conflict stations', 'wars war conflicts conflict throughout', 'wars war conflicts conflict throughout', 'war wars conflicts conflict play', 'war wars conflicts conflict play']\n",
      "['conflicts conflict wars war disputes', 'conflicts conflict wars war disputes', 'conflicts conflict wars war disputes', 'conflicts conflict wars war disputes', 'conflicts conflict wars war disputes']\n",
      "['<eos> honour quicker speedy recognised', '<eos> quicker speedy honour simplifying', '<eos> quicker speedy honour simplifying', '<eos> honour speedy quicker recognised', '<eos> quicker speedy honour simplifying']\n",
      "['life <eos> conflicts justice attain', '<eos> conflicts war us wars', '<eos> honour quicker speedy known', '<eos> tuna honour acquis condemns', '<eos> tuna life honour acquis']\n",
      "['<eos> life shipyards justice condemns', '<eos> war like well throughout', '<eos> life attain acquis tuna', '<eos> conflicts war conflict deplore', '<eos> quicker speedy glad well']\n",
      "reference-------------------------------\n",
      "war radio role radios play war armed conflict\n",
      "candidate-------------------------------\n",
      "radio wars role radio war conflicts honour conflicts radio wars role radio war conflicts honour war radio wars role radio war conflicts honour radio wars role radio war conflicts honour war conflicts <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> radio wars role radio war conflicts honour  \n",
      "uncovered-------------------------------\n",
      "{'radios', 'armed', 'play', 'conflict'}\n",
      "uncovered%-------------------------------\n",
      "0.5714285714285714\n",
      "******************************************\n",
      "spionaggio caso ames trovino documenti mostrano impatto caso spionaggio ames relazioni usa russia\n",
      "spionaggio caso ames trovino documenti mostrano impatto caso spionaggio ames relazioni usa russia\n",
      "translation table output\n",
      "#combine:0=0.08:1=0.08:2=0.08:3=0.08:4=0.08:5=0.08:6=0.08:7=0.08:8=0.08:9=0.08:10=0.08:11=0.08:12=0.08( #combine:0=0.43:1=0.23( espionage spying )#combine:0=0.66:1=0.12( case event )#combine:0=0.98( ames )#combine:0=0.33:1=0.04( find may )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.54:1=0.08( show shows )#combine:0=0.86:1=0.02( impact effect )#combine:0=0.66:1=0.12( case event )#combine:0=0.43:1=0.23( espionage spying )#combine:0=0.98( ames )#combine:0=0.47:1=0.38( relations reports )#combine:0=0.34:1=0.24( us usa )#combine:0=0.94:1=0.03( russia russian ))\n",
      "['case like cases happens affair', 'espionage regrettable tuberculosis interception tb', 'troika danish pnr derogations travellers', 'underline case danish <eos> troika', 'espionage tuberculosis regrettable tb scandals']\n",
      "['pnr speedy <eos> facilitating sv', 'found case cases find arise', 'case pnr <eos> sv speedy', 'recognises appreciates <eos> pnr like', '<eos> speedy sv pnr recognises']\n",
      "['case found show cases find', 'documents investigation information communication use', 'oblige sv favourable alde <eos>', 'documents information document evidence cases', 'speedy <eos> pnr quicker tampere']\n",
      "['show demonstrate prove shown showing', 'found show shown documents also', 'documents show document record papers', 'show demonstrate prove shown indicate', 'impact effect effects danger consequences']\n",
      "['impact effect effects consequences repercussions', 'show demonstrate prove indicate showing', 'impact effect effects consequences repercussions', 'case event epidemic infection outbreak', 'impact effects effect consequences repercussions']\n",
      "['case event espionage tuberculosis outbreak', 'impact effect effects consequences danger', 'case espionage event tuberculosis regrettable', 'case event espionage tuberculosis scandals', 'espionage regrettable interception pity innumerable']\n",
      "['case event espionage outbreak epidemic', 'espionage regrettable interception tuberculosis pity', 'espionage regrettable innumerable interception tuberculosis', '<eos> recognises authorisation case eib', 'facilitating troika enhancing profitability pnr']\n",
      "['<eos> facilitating recognises authorisation harmonise', 'facilitating recognises authorisation pnr harmonise', 'espionage regrettable interception tuberculosis pity', 'espionage regrettable interception innumerable tuberculosis', '<eos> recognises authorisation eib honour']\n",
      "['<eos> recognises authorisation facilitating harmonising', 'recognises authorisation facilitating harmonise tampere', 'favourable sv facilitating simplifying generalised', 'relations us relationship reports usa', 'relations reports us united usa']\n",
      "['usa united us american states', 'favourable sv facilitating generalised simplifying', 'relations reports us united usa', 'harmonising recognises authorisation <eos> harmonise', 'relations us relationship reports usa']\n",
      "['harmonise harmonised pnr facilitating preconditions', 'states nations countries state usa', 'usa united us uses american', 'analyse <eos> pnr simplifying recognises', 'usa united us uses american']\n",
      "['<eos> harmonise recognises simplifying recognising', 'harmonise harmonised facilitating authorisation preconditions', 'states nations countries state usa', 'analyse <eos> recognises simplifying clarification', 'harmonise harmonised authorisation pnr preconditions']\n",
      "['<eos> harmonise recognises simplifying recognising', 'favourable facilitating simplifying sv generalised', '<eos> information reports documents relations', '<eos> preconditions speedy pnr infrastructures', 'relations reports documents practices information']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favourable facilitating simplifying sv generalised', '<eos> european harmonising recognises human', '<eos> reports information documents relations', '<eos> preconditions speedy authorisation infrastructures', 'relations reports practices documents relationship']\n",
      "['favourable tampere facilitating simplifying attain', '<eos> european human us harmonising', '<eos> us logistics concerning concerned', '<eos> relations reports information case', 'favourable inquiries oblige facilitating attain']\n",
      "reference-------------------------------\n",
      "ames espionage case find documents show impact ames espionage case russian relations\n",
      "candidate-------------------------------\n",
      "espionage case pnr found documents show impact case espionage facilitating relations united states simplifying relations espionage case pnr found documents show impact case espionage facilitating relations united states simplifying reports espionage case facilitating documents show impact case espionage facilitating relations united states simplifying relations espionage case facilitating documents show impact case espionage facilitating relations united states simplifying relations espionage case facilitating documents show impact case espionage facilitating relations united states simplifying relations  \n",
      "uncovered-------------------------------\n",
      "{'find', 'ames', 'russian'}\n",
      "uncovered%-------------------------------\n",
      "0.3333333333333333\n",
      "******************************************\n",
      "vittorie alberto tomba trova documenti riportano vittorie alberto tomba gare sci\n",
      "vittorie alberto tomba trova documenti riportano vittorie alberto tomba gare sci\n",
      "translation table output\n",
      "#combine:0=0.09:1=0.09:2=0.09:3=0.09:4=0.09:5=0.09:6=0.09:7=0.09:8=0.09:9=0.09:10=0.09( #combine:0=0.64:1=0.06( victories victory )#combine:0=0.64:1=0.29( alberto albert )#combine:0=0.40:1=0.20( grave tomb )#combine:0=0.17:1=0.09( finds find )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.07:1=0.06( back bring )#combine:0=0.64:1=0.06( victories victory )#combine:0=0.64:1=0.29( alberto albert )#combine:0=0.40:1=0.20( grave tomb )#combine:0=0.12:1=0.11( invitations tenders )#combine:0=0.56:1=0.17( ski skiing ))\n",
      "['<eos> electors regards guaranteeing combating', '<eos> electors sheep regards combating', '<eos> regards electors sheep combating', 'victories failures matches commentators duties', '<eos> electors regards exportation guaranteeing']\n",
      "['<eos> orwell protects queiro electors', 'grave serious multitude <eos> blatant', '<eos> electors regards guaranteeing sheep', '<eos> queiro orwell alia stenmarck', '<eos> guaranteeing transmission electors regards']\n",
      "['<eos> orwell protects queiro qaeda', 'documents found find still document', 'documents found document reports still', '<eos> queiro alia orwell stenmarck', 'documents reports papers document people']\n",
      "['<eos> orwell queiro protects qaeda', 'reporting contain mention record find', 'reporting contain mention find show', 'reporting contain mention record find', 'documents record papers document reports']\n",
      "['<eos> orwell queiro protects qaeda', 'reporting mention contain refer apologise', 'victories organisational incompetence commentators collegiate', 'victories organisational incompetence failures collegiate', 'serious grave still documents found']\n",
      "['<eos> orwell queiro protects vis', 'victories organisational collegiate incompetence failures', 'victories organisational commentators failures compliant', 'serious found still grave documents', 'serious reports found documents report']\n",
      "['<eos> orwell queiro protects alia', 'serious found grave still <eos>', 'serious reports found documents report', 'multitude innumerable <eos> multiplicity grave', 'multitude grave multiplicity serious <eos>']\n",
      "['<eos> orwell queiro protects alia', 'serious found grave still <eos>', 'reports found serious documents report', 'multitude <eos> innumerable genuine multiplicity', 'multitude grave innumerable serious <eos>']\n",
      "['<eos> protects queiro orwell alia', 'serious found still find documents', 'reports found serious documents still', 'multitude <eos> innumerable us genuine', 'multitude grave innumerable serious <eos>']\n",
      "['<eos> protects queiro orwell alia', 'multitude <eos> innumerable us mrs', 'found reports serious documents still', 'found serious find still grave', 'multitude grave innumerable serious <eos>']\n",
      "['<eos> protects queiro orwell alia', 'multitude <eos> innumerable mrs us', 'found reports serious find still', 'found serious find still grave', 'multitude grave innumerable serious <eos>']\n",
      "['<eos> protects queiro orwell alia', 'multitude <eos> mrs innumerable us', 'found reports serious find still', 'found serious find still received', 'grave multitude innumerable <eos> serious']\n",
      "['<eos> protects orwell queiro alia', 'multitude <eos> mrs grave us', 'found serious reports find still', 'found serious find still <eos>', 'multitude grave innumerable <eos> electors']\n",
      "['<eos> protects orwell queiro alia', 'multitude grave homosexuality <eos> genuine', 'serious found find still grave', 'found serious reports find still', 'multitude grave innumerable unelected record']\n",
      "['<eos> protects orwell alia queiro', 'multitude grave homosexuality multiplicity <eos>', 'serious found find still grave', 'found serious reports find grave', 'multitude grave innumerable unelected record']\n",
      "['<eos> protects alia orwell vis', 'grave multitude homosexuality multiplicity serious', 'found serious find still grave', 'multitude grave innumerable unelected record', 'found reports serious find still']\n",
      "['<eos> alia protects vis orwell', 'grave multitude homosexuality serious multiplicity', 'multitude grave innumerable serious veritable', 'found serious find still received', 'grave multitude innumerable multiplicity <eos>']\n",
      "['<eos> alia vis protects orwell', 'grave multitude serious innumerable genuine', 'grave multitude serious homosexuality multiplicity', 'grave multitude multiplicity innumerable orwell', 'found serious find still <eos>']\n",
      "reference-------------------------------\n",
      "victories alberto tomba find documents report ski races won alberto tomba\n",
      "candidate-------------------------------\n",
      "victories victories victories victories victories  \n",
      "uncovered-------------------------------\n",
      "{'races', 'alberto', 'find', 'documents', 'won', 'ski', 'report', 'tomba'}\n",
      "uncovered%-------------------------------\n",
      "0.8888888888888888\n",
      "******************************************\n",
      "conflitto interessi italia recupera documenti parla problema conflitto interessi presidente consiglio ministri italiano silvio berlusconi\n",
      "conflitto interessi italia recupera documenti parla problema conflitto interessi presidente consiglio ministri italiano silvio berlusconi\n",
      "translation table output\n",
      "#combine:0=0.07:1=0.07:2=0.07:3=0.07:4=0.07:5=0.07:6=0.07:7=0.07:8=0.07:9=0.07:10=0.07:11=0.07:12=0.07:13=0.07:14=0.07( #combine:0=0.84:1=0.03( conflict war )#combine:0=0.85:1=0.10( interests interest )#combine:0=0.93:1=0.03( italy italian )#combine:0=0.10:1=0.06( back recovering )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.19:1=0.13( talk talking )#combine:0=0.71:1=0.13( problem issue )#combine:0=0.84:1=0.03( conflict war )#combine:0=0.85:1=0.10( interests interest )#combine:0=0.91:1=0.04( president mr )#combine:0=0.96:1=0.01( council office )#combine:0=0.93:1=0.01( ministers minister )#combine:0=0.84:1=0.07( italian italy )#combine:0=0.60:1=0.18( silvio mr )#combine:0=0.71:1=0.18( berlusconi mr ))\n",
      "['interests interest italy italian concerns', 'interests interest italy italian concern', 'conflict conflicts interests disputes opposition', 'conflict conflicts interests interest disputes', 'interests italy interest italian concerns']\n",
      "['italy italian italians turkey detained', 'italy italian interests turkey france', 'interests interest interested sake whose', 'italy italian italians like turkey', 'interests interest interested concerns concern']\n",
      "['returns will recover return recovered', 'returns will recover return recovered', 'returns will recover return repatriation', 'returns will recover return recovered', 'returns will return recover repatriation']\n",
      "['documents document papers paper texts', 'recover return returns recovered regain', 'documents paper papers document texts', 'documents papers document paper texts', 'documents document papers paper texts']\n",
      "['talks talk talking speaks mention', 'talks talk talking speaks mention', 'talks talk talking speaks question', 'documents paper papers document texts', 'talks talk talking speaks mention']\n",
      "['conflict problem issue dispute conflicts', 'conflict problem issue dispute conflicts', 'conflict problem issue dispute question', 'talks talk talking speaks mention', 'conflict problem issue dispute conflicts']\n",
      "['interests interest concerns concern problem', 'conflict conflicts dispute war fighting', 'interests interest concerns concern problem', 'conflict problem issue dispute conflicts', 'interests interest concerns concern italian']\n",
      "['italian president italy prime council', 'interests interest concerns italian concern', 'italian president italy prime council', 'italian president italy prime council', 'italian president interests mr italy']\n",
      "['prime minister council president ministers', 'italian president council italy prime', 'prime minister council president ministers', 'prime minister council president ministers', 'council office italian italy european']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minister ministers affairs prime ministry', 'prime minister council president ministers', 'mr council prodi romano turkish', 'ministers minister prime president italian', 'minister ministers affairs prime ministry']\n",
      "['mr prodi romano silvio council', 'minister ministers affairs prime ministry', 'mr prodi romano silvio council', 'mr prodi silvio romano berlusconi', 'council mr prodi romano prime']\n",
      "['mr prodi romano council silvio', 'prodi romano mountainous shares rumsfeld', 'prodi romano mr silvio berlusconi', 'ecb eec envisaged vigilant <eos>', 'prodi romano mountainous shares rumsfeld']\n",
      "['prodi romano mountainous shares rumsfeld', 'prodi romano silvio mr berlusconi', 'mr turkish <eos> italian council', 'eec decommissioning transposition envisaged vigilant', 'ecb eec envisaged <eos> vigilant']\n",
      "['mr <eos> turkish italian prime', 'eec decommissioning envisaged <eos> transposition', 'prodi romano rumsfeld valery shares', 'prodi romano mr silvio berlusconi', 'eec transposition envisaged decommissioning updating']\n",
      "['prodi romano rumsfeld valery ecb', '<eos> italian council mr minister', 'prodi qaeda giscard qa concentrates', 'mr prodi romano turkish italian', 'transposition eec decommissioning facilitating envisaged']\n",
      "['giscard prodi <eos> qaeda cypriot', '<eos> italian council mr minister', 'transposition eec decommissioning facilitating envisaged', '<eos> well spoke minister others', '<eos> council italian mr prime']\n",
      "['prodi giscard <eos> qaeda cypriot', '<eos> giscard qaeda prodi condemns', 'estaing scepticism grassroots annan defence', '<eos> italian council minister mr', 'minister ministers prime government council']\n",
      "reference-------------------------------\n",
      "conflict interests italy find documents discuss problem conflict interests italian prime minister silvio berlusconi\n",
      "candidate-------------------------------\n",
      "conflict interests italy returns documents talks conflict interests italian prime minister mr prodi mr prodi conflict interests italy returns documents talks problem conflict interests italian prime minister mr prodi mr prodi italian minister <unk> conflict interests italy returns documents talks problem conflict interests italian prime minister mr prodi mr prodi conflict interests italy returns documents talks problem conflict interests italian prime minister mr prodi mr prodi conflict interests italy returns documents talks problem conflict interests italian prime minister mr prodi mr prodi  \n",
      "uncovered-------------------------------\n",
      "{'find', 'berlusconi', 'discuss', 'silvio'}\n",
      "uncovered%-------------------------------\n",
      "0.3333333333333333\n",
      "******************************************\n",
      "medaglia oro super vinse medaglia oro super olimpiadi invernali lillehammer\n",
      "medaglia oro super vinse medaglia oro super olimpiadi invernali lillehammer\n",
      "translation table output\n",
      "#combine:0=0.10:1=0.10:2=0.10:3=0.10:4=0.10:5=0.10:6=0.10:7=0.10:8=0.10:9=0.10( #combine:0=0.48:1=0.21( coin side )#combine:0=0.59:1=0.30( gold golden )#combine:0=0.72:1=0.15( super superstate )#combine:0=0.51:1=0.08( won votes )#combine:0=0.48:1=0.21( coin side )#combine:0=0.59:1=0.30( gold golden )#combine:0=0.72:1=0.15( super superstate )#combine:0=0.33:1=0.33( games olympic )#combine:0=0.83:1=0.04( winter cold )#combine:0=0.98:1=0.00( lillehammer setting ))\n",
      "['coin coins sphere counterfeiting liberalism', 'coin coins liberalism counterfeiting currency', 'golden gold coins coin currency', 'golden gold coins coin currency', 'golden gold coins coin currency']\n",
      "['super superstate utopian convincing currency', 'super superstate utopian convincing single', 'coin coins liberalism blue currencies', 'coin coins liberalism blue currencies', 'super currency convincing myth superstate']\n",
      "['super free won blue winning', 'super free won currency blue', 'won winning gained victory wins', 'super superstate convincing utopian convince', 'super superstate utopian convincing convince']\n",
      "['won winning free victory blue', 'golden gold coins coin currencies', 'won winning win victory free', 'won free winning super victory', 'won free winning super victory']\n",
      "['golden gold coin coins currency', 'coin coins liberalism sphere currency', 'golden gold coin coins currencies', 'coin coins liberalism currency counterfeiting', 'golden gold coins coin currencies']\n",
      "['winter super cold olympic millennium', 'coin coins liberalism currency sphere', 'winter super cold utopian convince', 'coin coins liberalism currency counterfeiting', 'coin coins liberalism currency sphere']\n",
      "['winter super cold olympic millennium', 'olympic olympics round ice winter', 'winter super cold utopian utopia', 'winter super cold olympic millennium', 'olympic olympics ice round winter']\n",
      "['super olympic beautiful superiority olympics', 'empt <eos> quicker harmonise speedy', 'olympic olympics ice round winter', 'olympic olympics round ice winter', 'super olympic beautiful superiority games']\n",
      "['super olympic beautiful games olympics', 'empt <eos> quicker speedy harmonise', 'olympic super beautiful olympics games', 'empt <eos> quicker speedy harmonise', 'empt <eos> quicker authorisation speedy']\n",
      "['empt quicker <eos> authorisation harmonise', 'super olympic beautiful convince games', '<eos> speedy harmonise quicker empt', 'super olympic beautiful games convince', 'super superstate convince utopian convincing']\n",
      "['empt <eos> quicker authorisation speedy', 'empt authorisation quicker roadmap authorisations', 'empt harmonise <eos> speedy quicker', '<eos> speedy quicker harmonise empt', 'empt authorisation quicker roadmap authorisations']\n",
      "['<eos> speedy quicker harmonise empt', '<eos> speedy harmonise envisages quicker', '<eos> speedy harmonise speeded quicker', 'olympic <eos> cmo trips olympics', 'olympic olympics <eos> millennium globalised']\n",
      "reference-------------------------------\n",
      "super gold medal won gold medal super lillehammer olympic winter games\n",
      "candidate-------------------------------\n",
      "golden coin super free won golden coin winter olympic beautiful super golden coin super free won golden coin winter olympic olympic super empt golden coin super free won golden coin winter olympic beautiful super empt golden coin super free won golden coin winter olympic beautiful superstate empt golden coin super free won golden coin winter olympic super empt  \n",
      "uncovered-------------------------------\n",
      "{'games', 'gold', 'lillehammer', 'medal'}\n",
      "uncovered%-------------------------------\n",
      "0.5\n",
      "******************************************\n",
      "asma bronchiale trova documenti parlano asma bronchiale nuove scoperte fatte medicina malattia respiratoria\n",
      "asma bronchiale trova documenti parlano asma bronchiale nuove scoperte fatte medicina malattia respiratoria\n",
      "translation table output\n",
      "#combine:0=0.08:1=0.08:2=0.08:3=0.08:4=0.08:5=0.08:6=0.08:7=0.08:8=0.08:9=0.08:10=0.08:11=0.08:12=0.08( #combine:0=0.77:1=0.04( asthma causes )#combine:0=0.49:1=0.25( bronchial prisoners )#combine:0=0.17:1=0.09( finds find )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.29:1=0.17( speak talk )#combine:0=0.77:1=0.04( asthma causes )#combine:0=0.49:1=0.25( bronchial prisoners )#combine:0=0.94:1=0.01( new fresh )#combine:0=0.33:1=0.24( discoveries findings )#combine:0=0.66:1=0.06( made done )#combine:0=0.70:1=0.14( medicine medical )#combine:0=0.66:1=0.12( disease illness )#combine:0=0.54:1=0.05( respiratory breathing ))\n",
      "['asthma leukaemia hepatitis salmonella scrapie', 'salmonella undesirable globalised unwanted leukaemia', 'asthma leukaemia hepatitis salmonella undesirable', 'asthma leukaemia salmonella hepatitis undesirable', '<eos> provisional compulsory freezing ageing']\n",
      "['found find documents contains also', 'found find documents also still', 'found contains <eos> find still', 'found find documents also still', 'asthma leukaemia undesirable hepatitis salmonella']\n",
      "['documents papers document languages contain', 'documents papers languages contain document', 'documents papers document one paper', 'documents papers document one paper', 'documents papers languages document contain']\n",
      "['mention speak refer talk talking', 'mention speak talk refer talking', 'mention speak talk refer talking', 'mention speak refer talk talking', 'mention speak refer talk talking']\n",
      "['multiannual provisional compulsory lastly freezing', 'provisional multiannual compulsory lastly freezing', 'multiannual provisional compulsory lastly freezing', 'provisional multiannual compulsory lastly freezing', 'provisional multiannual lastly compulsory <eos>']\n",
      "['leukaemia asthma scrapie hepatitis salmonella', 'asthma leukaemia undesirable hepatitis salmonella', 'asthma leukaemia salmonella scrapie hepatitis', 'provisional multiannual <eos> compulsory freezing', 'asthma leukaemia undesirable salmonella scrapie']\n",
      "['new latest fresh novel discoveries', 'new latest fresh discoveries novel', 'new discoveries fresh latest novel', 'new discoveries fresh latest novel', 'new latest fresh novel discoveries']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discoveries findings exposure discovery summary', 'discoveries findings exposure discovery summary', 'discoveries findings exposure discovery summary', 'discoveries findings exposure discovery summary', 'discoveries findings exposure discovery summary']\n",
      "['medicine made medical medicines drugs', 'medicine made medical medicines carried', 'medicine made medical medicines carried', 'medicine made medical medicines drugs', 'medicine made medical medicines drugs']\n",
      "['disease illness bse diseases illnesses', 'disease illness diseases bse illnesses', 'disease illness diseases bse illnesses', 'disease illness bse diseases illnesses', 'disease illness diseases illnesses bse']\n",
      "['speedy authorisation eradication facilitating preconditions', 'speedy authorisation eradication facilitating preconditions', 'speedy authorisation eradication preconditions facilitating', 'speedy authorisation eradication facilitating preconditions', 'speedy authorisation eradication preconditions facilitating']\n",
      "['disease <eos> alzheimer health illnesses', '<eos> speedy medicinal optimal probability', 'disease alzheimer <eos> health illnesses', '<eos> speedy medicinal optimal probability', '<eos> speedy quicker harmonise favourable']\n",
      "['speedy <eos> eradication authorisation labelling', 'speedy <eos> eradication authorisation labelling', 'favourable <eos> autumn inquiries beneficiaries', 'favourable <eos> autumn inquiries stabilisation', 'favourable <eos> inquiries autumn stabilisation']\n",
      "['disease <eos> treatment health spread', 'disease <eos> treatment health life', 'favourable inquiries <eos> facilitating elapsed', '<eos> speedy harmonise sis inequality', 'favourable inquiries <eos> facilitating elapsed']\n",
      "['<eos> eradication speedy labelling optimal', '<eos> eradication speedy labelling optimal', 'favourable <eos> inquiries autumn stabilisation', 'favourable <eos> inquiries autumn elapsed', 'favourable <eos> inquiries autumn elapsed']\n",
      "['favourable inquiries <eos> autumn facilitating', 'favourable <eos> inquiries autumn elapsed', '<eos> life disease additives neoliberal', '<eos> disease treatment use light', '<eos> life disease additives neoliberal']\n",
      "reference-------------------------------\n",
      "bronchial asthma find documents talk bronchial asthma recent medical discoveries concerning respiratory disease\n",
      "candidate-------------------------------\n",
      "compulsory leukaemia found documents mention multiannual leukaemia new discoveries medicine disease speedy disease speedy disease eradication compulsory leukaemia found documents mention provisional asthma new discoveries medicine disease speedy disease speedy disease eradication compulsory leukaemia found documents mention multiannual leukaemia new discoveries medicine disease speedy disease speedy disease speedy compulsory leukaemia found documents mention multiannual leukaemia new discoveries medicine disease speedy disease speedy disease compulsory leukaemia found documents mention provisional asthma new discoveries medicine disease speedy disease speedy disease  \n",
      "uncovered-------------------------------\n",
      "{'recent', 'find', 'respiratory', 'bronchial', 'medical', 'talk', 'concerning'}\n",
      "uncovered%-------------------------------\n",
      "0.6363636363636364\n",
      "******************************************\n",
      "industria automobilistica europa trovino documenti parlano situazione industria automobilistica europa calo vendite crisi mercato eventuali contromisure\n",
      "industria automobilistica europa trovino documenti parlano situazione industria automobilistica europa calo vendite crisi mercato eventuali contromisure\n",
      "translation table output\n",
      "#combine:0=0.06:1=0.06:2=0.06:3=0.06:4=0.06:5=0.06:6=0.06:7=0.06:8=0.06:9=0.06:10=0.06:11=0.06:12=0.06:13=0.06:14=0.06:15=0.06( #combine:0=0.90:1=0.02( industry industries )#combine:0=0.46:1=0.16( car motor )#combine:0=0.92:1=0.04( europe european )#combine:0=0.33:1=0.04( find may )#combine:0=0.79:1=0.08( documents papers )#combine:0=0.29:1=0.17( speak talk )#combine:0=0.85:1=0.02( situation state )#combine:0=0.90:1=0.02( industry industries )#combine:0=0.46:1=0.16( car motor )#combine:0=0.92:1=0.04( europe european )#combine:0=0.16:1=0.13( fall decline )#combine:0=0.73:1=0.10( sales selling )#combine:0=0.88:1=0.08( crisis crises )#combine:0=0.95:1=0.02( market markets )#combine:0=0.56:1=0.11( possible potential )#combine:0=0.24:1=0.22( countermeasures measures ))\n",
      "['industry manufacturing manufacturers industries car', 'industry vehicle car vehicles industries', 'industry sector industries manufacturing manufacturers', 'industry industries manufacturing sector manufacturers', 'car motor automotive automobile vehicle']\n",
      "['europe found within european across', 'europe within european found across', 'europe within european found across', 'industry europe sector industries european', 'industry industries manufacturing manufacturers sector']\n",
      "['find found will documents finding', 'find found documents will finding', 'find found will documents finding', 'europe within found european across', 'europe within found european across']\n",
      "['documents paper papers document us', 'documents paper papers document record', 'documents paper papers document one', 'documents paper papers document one', 'find found documents will come']\n",
      "['talk speak talking mention refer', 'talk speak talking mention refer', 'talk talking situation speak discussing', 'talk speak talking mention refer', 'talk speak talking mention speaking']\n",
      "['situation position car european conditions', 'situation position car european conditions', 'situation position car conditions european', 'situation position car european conditions', 'situation position european car state']\n",
      "['car motor automotive automobile european', 'car motor automotive automobile european', 'car motor automotive automobile european', 'car motor automotive automobile european', 'car motor automotive automobile european']\n",
      "['industry manufacturing industries manufacturers sector', 'industry manufacturing industries manufacturers sector', 'industry manufacturing industries manufacturers sector', 'industry manufacturing industries manufacturers sector', 'industry manufacturing industries manufacturers sector']\n",
      "['europe across within throughout european', 'europe across within throughout european', 'europe across within throughout european', 'europe across within throughout european', 'europe across within throughout european']\n",
      "['falling fall decline drop declining', 'falling fall decline drop declining', 'falling fall decline drop declining', 'falling fall decline drop declining', 'falling fall decline drop declining']\n",
      "['sales selling market sale prices', 'sales selling market sale prices', 'sales selling market sale prices', 'sales selling market sale prices', 'sales selling market sale prices']\n",
      "['crisis market crises markets price', 'crisis market crises markets price', 'crisis market crises markets price', 'crisis market crises markets price', 'crisis market crises markets price']\n",
      "['market markets like possible within', 'crisis crises market price risk', 'market markets like possible within', 'market markets like possible within', 'crisis crises market price risk']\n",
      "['possible <eos> may accessing like', 'possible <eos> may accessing like', 'possible <eos> may like well', 'possible <eos> may like accessing', 'possible <eos> may like well']\n",
      "['<eos> speedy harmonised quicker alde', '<eos> speedy harmonised quicker alde', 'favourable presupposes inquiries oblige qaeda', '<eos> speedy harmonised quicker alde', '<eos> speedy harmonised alde quicker']\n",
      "['favourable inquiries <eos> oblige presupposes', '<eos> sales rise loss supply', 'favourable inquiries oblige <eos> presupposes', '<eos> sales rise loss supply', '<eos> tonnes maximise observations efficacy']\n",
      "['presupposes qaeda <eos> favourable inquiries', 'qaeda presupposes inquiries favourable oblige', 'qaeda presupposes oblige favourable inquiries', '<eos> optimal observations optimise favourable', '<eos> people healthcare ecb observations']\n",
      "['<eos> presupposes inquiries qaeda redundancies', 'presupposes qaeda oblige inquiries <eos>', '<eos> industry sector industries market', '<eos> speedy public tonnes rights', '<eos> rights people tonnes speedy']\n",
      "reference-------------------------------\n",
      "european car industry find documents report situation european car industry regarding fall sales sales crisis possible countermeasures\n",
      "candidate-------------------------------\n",
      "car industry europe find documents talk situation car industry europe falling sales crisis market possible speedy car industry europe find documents talk situation car industry europe falling sales crisis market possible harmonised car industry europe find documents talk situation car industry europe falling sales crisis market possible speedy car industry europe find documents talk situation car industry europe falling sales crisis market possible car industry europe find documents talk situation car industry europe falling sales crisis market possible  \n",
      "uncovered-------------------------------\n",
      "{'fall', 'european', 'report', 'regarding', 'countermeasures'}\n",
      "uncovered%-------------------------------\n",
      "0.38461538461538464\n",
      "******************************************\n",
      "ingegneria genetica modo ingegneria genetica influisce catena alimentare uomo\n",
      "ingegneria genetica modo ingegneria genetica influisce catena alimentare uomo\n",
      "translation table output\n",
      "#combine:0=0.11:1=0.11:2=0.11:3=0.11:4=0.11:5=0.11:6=0.11:7=0.11:8=0.11( #combine:0=0.77:1=0.10( engineering technology )#combine:0=0.66:1=0.13( genetic genetics )#combine:0=0.63:1=0.07( way ensure )#combine:0=0.77:1=0.10( engineering technology )#combine:0=0.66:1=0.13( genetic genetics )#combine:0=0.30:1=0.08( affects influence )#combine:0=0.87:1=0.03( chain knock )#combine:0=0.89:1=0.01( food foodstuffs )#combine:0=0.76:1=0.14( human man ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['engineering genetic innovation gene technology', 'technology engineering genetic technological innovation', 'genetic gene way also ecosystem', 'engineering genetic technology gene ecosystem', 'genetic gene genes way ecosystem']\n",
      "['genetic gene way ecosystem genes', 'genetic gene way genes ecosystem', 'genetic gene way ecosystem means', 'engineering technology innovation genetic technological', 'genetic engineering gene technology technological']\n",
      "['engineering genetic innovation gene technology', 'technology engineering genetic technological technologies', 'genetic gene ecosystem genes embryonic', 'engineering genetic technology innovation gene', 'engineering genetic technology innovation technological']\n",
      "['affects affecting affect way impact', 'affects affecting affect impact also', 'engineering genetic technology innovation gene', 'affects affecting affect impact way', 'affects affecting affect impact way']\n",
      "['food feed foodstuffs chain foods', 'affects affecting affect impact influence', 'food feed foodstuffs chain world', 'food feed foodstuffs chain world', 'genetic gene ecosystem influenza avian']\n",
      "['chain supply chains supplies feed', 'chain supply chains feed supplies', 'food feed foodstuffs chain foods', 'chain supply chains feed supplies', 'engineering genetic technology innovation gene']\n",
      "['<eos> enhancing biofuels employability speedy', '<eos> enhancing biofuels recognises genetically', 'chain supply chains supplies feed', '<eos> enhancing biofuels recognises employability', 'affects affecting affect impact influence']\n",
      "['<eos> enhancing biofuels speedy favourable', 'food feed foodstuffs chain us', 'food feed foodstuffs chain world', '<eos> biofuels judgement oblige stresses', 'food feed foodstuffs chain world']\n",
      "['chain supply chains feed supplies', 'chain supply chains feed supplies', 'chain supply chains feed supplies', '<eos> biofuels stresses judgement oblige', 'food genetic genetically world quality']\n",
      "['<eos> enhancing empt recognises favourable', '<eos> enhancing empt recognises favourable', '<eos> enhancing recognises speedy favourable', 'chain supply supplies chains additives', '<eos> concludes stresses judgement oblige']\n",
      "['<eos> biofuels oblige stresses eib', '<eos> biofuels oblige stresses eib', '<eos> biofuels oblige stresses eib', 'food world genetically quality human', '<eos> food genetic fact life']\n",
      "reference-------------------------------\n",
      "genetic engineering genetic engineering effect human food chain\n",
      "candidate-------------------------------\n",
      "genetic engineering genetic engineering way genetic engineering affects food chain enhancing food <unk> <unk> <unk> <unk> <unk> <unk> <unk> genetic engineering genetic engineering way genetic engineering affects food chain genetic engineering genetic engineering way genetic engineering affects food chain genetic engineering genetic engineering way genetic engineering affects food chain genetic engineering genetic engineering way genetic engineering affects food chain  \n",
      "uncovered-------------------------------\n",
      "{'effect', 'human'}\n",
      "uncovered%-------------------------------\n",
      "0.3333333333333333\n",
      "******************************************\n",
      "secessione yemen sud quali conseguenze economiche sociali dichiarazione secessione yemen sud\n",
      "secessione yemen sud quali conseguenze economiche sociali dichiarazione secessione yemen sud\n",
      "translation table output\n",
      "#combine:0=0.09:1=0.09:2=0.09:3=0.09:4=0.09:5=0.09:6=0.09:7=0.09:8=0.09:9=0.09:10=0.09( #combine:0=0.70:1=0.04( secession break )#combine:0=0.91:1=0.03( yemen yemeni )#combine:0=0.78:1=0.17( south southern )#combine:0=0.26:1=0.22( including like )#combine:0=0.70:1=0.07( consequences effects )#combine:0=0.91:1=0.02( economic financial )#combine:0=0.93:1=0.01( social society )#combine:0=0.47:1=0.39( statement declaration )#combine:0=0.70:1=0.04( secession break )#combine:0=0.91:1=0.03( yemen yemeni )#combine:0=0.78:1=0.17( south southern ))\n",
      "['yemen kenya macedonia papua south', 'karabakh vis annan nord censure', 'yemen kenya south macedonia indonesia', 'yemen south oil southern arab', 'yemen kenya macedonia countries indonesia']\n",
      "['south southern also west even', 'yemen south southern oil macedonia', 'south southern also even west', 'south southern also even including', 'south southern also even including']\n",
      "['economic consequences social also financial', 'south southern also economic even', 'economic consequences financial also social', 'economic consequences social also whose', 'economic consequences also social will']\n",
      "['social consequences well political socially', 'economic consequences also financial whose', 'social consequences well political socially', 'social consequences well political socially', 'social consequences well political socially']\n",
      "['consequences implications effects repercussions impact', 'social consequences well political monetary', 'consequences implications repercussions effects impact', 'consequences implications repercussions effects impact', 'consequences implications effects repercussions impact']\n",
      "['declaration statement uzbekistan macedonia hague', 'consequences implications effects repercussions impact', 'declaration statement uzbekistan macedonia signed', 'declaration statement uzbekistan macedonia hague', 'declaration statement uzbekistan macedonia signed']\n",
      "['declaration statement uzbekistan macedonia hague', 'secession imperialist shores nagorno rockets', 'secession iraqi imperialist shores lebanese', 'secession imperialist shores nagorno rockets', 'secession imperialist shores nagorno rockets']\n",
      "['secession imperialist nagorno shores rockets', 'yemen indonesia kenya vigilance macedonia', 'secession iraqi imperialist lebanese nagorno', 'yemen indonesia kenya vigilance macedonia', 'yemen indonesia kenya vigilance macedonia']\n",
      "['<eos> alde harmonise harmonisation speedy', 'yemen indonesia vigilance kenya macedonia', '<eos> alde harmonise harmonisation speedy', '<eos> alde harmonise harmonisation speedy', 'yemen indonesia vigilance kenya macedonia']\n",
      "['<eos> alde harmonise introductory speedy', '<eos> alde introductory harmonise speedy', 'oblige qaeda accede generalised fifthly', 'group <eos> people leaders groups', '<eos> people alde preconditions rights']\n",
      "['<eos> will well like recognises', 'qaeda oblige generalised neighbours fifthly', 'group <eos> people leaders groups', '<eos> people rights peoples human', 'remarks statement speech statements visit']\n",
      "['qaeda <eos> fifthly oblige neighbours', '<eos> will well like recognises', '<eos> analyse speedy envisages harmonise', '<eos> lithuanian speedy introductory envisages', '<eos> speedy introductory lithuanian recognises']\n",
      "['qaeda neighbours <eos> fifthly oblige', 'qaeda oblige neighbours <eos> fifthly', '<eos> well will others also', 'qaeda oblige generalised alde neighbours', 'qaeda <eos> oblige fifthly neighbours']\n",
      "['qaeda <eos> qa neighbours oblige', '<eos> well will also others', '<eos> well like will south', 'qaeda neighbours <eos> oblige fifthly', '<eos> declaration statement will action']\n",
      "['qaeda neighbours qa <eos> fifthly', 'qaeda qa neighbours <eos> fifthly', '<eos> well will consequences also', 'qaeda neighbours qa fifthly <eos>', 'qaeda <eos> neighbours qa oblige']\n",
      "['qaeda <eos> qa neighbours fifthly', '<eos> well will consequences also', '<eos> will also leaders well', '<eos> south well like will', 'ida masse blanche roughshod rouge']\n",
      "['qaeda neighbours qa annan <eos>', '<eos> conference declaration will summit', 'qaeda neighbours qa annan neighbour', '<eos> consequences will well also', 'qaeda neighbours qa annan pillars']\n",
      "['qaeda neighbours qa <eos> annan', '<eos> will well consequences leaders', 'qaeda <eos> neighbours annan qa', '<eos> will also leaders well', '<eos> south well like will']\n",
      "reference-------------------------------\n",
      "southern yemen secession social economic consequences declaration secession southern yemen\n",
      "candidate-------------------------------\n",
      "nagorno karabakh yemen south economic social consequences declaration secession yemen alde group nagorno karabakh yemen south economic social consequences declaration secession yemen alde group nagorno karabakh yemen south economic social consequences declaration secession yemen alde group nagorno karabakh yemen south economic social consequences declaration secession yemen alde group nagorno karabakh yemen south economic social consequences declaration secession yemen alde group  \n",
      "uncovered-------------------------------\n",
      "{'southern'}\n",
      "uncovered%-------------------------------\n",
      "0.14285714285714285\n",
      "******************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of uncovered terms 0.43741536241536244\n",
      "sentence\t5\t1\t0.1185\n"
     ]
    }
   ],
   "source": [
    "def load_query_file(qdir, qtitle, qdesc):\n",
    "    #passing query directory, query title and query title description\n",
    "    #Creating query_dict where title and query against a query id would be available\n",
    "    query_file_title = open(os.path.join(qdir, qtitle))\n",
    "    query_dict = {}\n",
    "    query_title_len_dict = {}\n",
    "    for line in query_file_title:\n",
    "        line_splitted = line.split(\"\\t\")\n",
    "        query_id = line_splitted[0].strip()\n",
    "        query_title = line_splitted[1].strip()\n",
    "        query_dict.setdefault(query_id, [])\n",
    "        query_dict[query_id].append(query_title)\n",
    "        query_title_len_dict[query_id] = len(query_title.split())\n",
    "        #print(line)\n",
    "    query_file_desc = open(os.path.join(qdir, qdesc))\n",
    "    for line in query_file_desc:\n",
    "        line_splitted = line.split(\"\\t\")\n",
    "        query_id = line_splitted[0].strip()\n",
    "        query_title_desc = line_splitted[1].strip()\n",
    "        query_dict[query_id].append(query_title_desc)\n",
    "        title_len = query_title_len_dict[query_id]\n",
    "        query_desc = ' '.join(query_title_desc.split()[title_len: ])\n",
    "        query_dict[query_id].append(query_desc)\n",
    "    return query_dict\n",
    "\n",
    "\n",
    "#Query dict created\n",
    "#Order in which queries are put: title, title_description, description\n",
    "#load italian queries\n",
    "verbose = True\n",
    "single_test = True \n",
    "eng_query_dict = load_query_file(\"CLEF-ENG-ML/index/english/all\", \"All-eng-tit-final.tsv\", \"All-eng-tit-des-final-clef.tsv\")\n",
    "it_query_dict = load_query_file(\"CLEF-ENG-ML/index/italian\", \"All-Top-ita-tit-final.txt\", \"All-Top-ita-tit-desc-final.txt\")\n",
    "it_query_tt_dict = load_query_tt(\"CLEF-ENG-ML/index/italian\", \"italian_tt.txt\")\n",
    "print(model_name)\n",
    "print(\"parallel sentences\\tweight neural\\tMAP\")\n",
    "print(\"****************************************************\")\n",
    "\n",
    "if single_test == True:\n",
    "    params = {'k':[5], 'weight': [1]}\n",
    "else:\n",
    "    params = {'k': range(5, 10), 'weight': np.linspace(0, 1, 5)}        \n",
    "    \n",
    "settings = {1:'sentence'}\n",
    "for setting in settings.keys(): \n",
    "    for k in params['k']:\n",
    "        for weight in params['weight']:\n",
    "            opt.k = k \n",
    "            translation_file = open(os.path.join(\"CLEF-ENG-ML/index/english/all\", \"All-eng-tit-des-final.tsv\"), \"w\")\n",
    "            sum_percentage_uncovered = 0.0\n",
    "            list_of_references = []\n",
    "            hypotheses = []\n",
    "            length = 0\n",
    "            for key in sorted(it_query_dict)[0:10]:    \n",
    "                length+=1\n",
    "                #print(key)\n",
    "                opt.text = it_query_dict[key][1] ### getting the italian query \n",
    "                print(opt.text)\n",
    "                tt_query = it_query_tt_dict[opt.text]\n",
    "\n",
    "                if verbose == True:\n",
    "                    print(opt.text)\n",
    "                    print(\"translation table output\")\n",
    "                    print(tt_query)\n",
    "                \n",
    "                candidate, queries, query_tokens = translate(opt, model, SRC, TRG)\n",
    "\n",
    "                if setting == 0:\n",
    "                    candidate = ' '.join(queries[0].keys()) ###\n",
    "                #else: \n",
    "                    #candidate = ' '.join(query_tokens)\n",
    "                #candidate+= ' '.join(queries[1].keys())\n",
    "                #candidate = candidate + \" \" + opt.text + \" \" + tt_query ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "                str_append = \" \"\n",
    "#                 for i in np.arange(opt.k):\n",
    "#                     str_append+=opt.text + \" \"\n",
    "\n",
    "                #if setting==1:\n",
    "                #for i in np.arange(opt.k):\n",
    "                    #str_append+=tt_query + \" \"\n",
    "                candidate = candidate + \" \" + str_append  ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "                #candidate = tt_query\n",
    "                candidate = candidate.lower() ###\n",
    "                \n",
    "                #candidate = create_galago_query(queries[0], opt.text + tt_query) ### weights estimated by neural approach. The code is put below\n",
    "                #candidate = create_galago_query(queries[0], \"\") ### weights estimated by neural approach. The code is put below\n",
    "                \n",
    "                reference = eng_query_dict[key][1] ###\n",
    "                list_of_references.append(reference)\n",
    "                hypotheses.append(candidate)\n",
    "                reference_splitted = set(reference.split())\n",
    "                candidate_splitted = set(candidate.split())\n",
    "                uncovered = reference_splitted.difference(candidate_splitted)\n",
    "                if verbose== True:\n",
    "                    print(\"reference-------------------------------\")\n",
    "                    print(reference)\n",
    "                    print(\"candidate-------------------------------\")\n",
    "                    print(candidate)\n",
    "                    print(\"uncovered-------------------------------\")\n",
    "                    print(uncovered)\n",
    "                    print(\"uncovered%-------------------------------\")\n",
    "                percentage_uncovered = len(uncovered)/(len(reference_splitted) * 1.0)\n",
    "                sum_percentage_uncovered+=percentage_uncovered \n",
    "\n",
    "                if verbose == True:\n",
    "                    print(str(percentage_uncovered))\n",
    "                    print(\"******************************************\")\n",
    "\n",
    "                covered = reference_splitted.difference(uncovered)\n",
    "                str_uncovered = ' '.join(uncovered)\n",
    "                str_covered = ' '.join(covered)    \n",
    "                translation_file.write(key + \"\\t\" + candidate + \" \\n\")    \n",
    "                #translation_file.write(key + \"\\t\" + tt_query + \" \\n\")            \n",
    "                #translation_file.write(key + \"\\t\" + reference + \" \\n\")        \n",
    "                #translation_file.write(key + \"\\t\" + \"#combine:0=\" + str(weight)+ \":1=\" + str(1-weight) + \"(#combine(\" + candidate + \") #combine(\" + tt_query + \"))\\n\")        \n",
    "                #translation_file.write(key + \"\\t\" + candidate + \" \" + reference + \" \\n\")    \n",
    "                #translation_file.write(key + \"\\t\" + \" \" + str_covered + \" \\n\")    \n",
    "                #translation_file.write(key + \"\\t\" + \" \" +  + \" \" + str_uncovered + \" \\n\")    \n",
    "\n",
    "            translation_file.close() \n",
    "            # Check current working directory.\n",
    "            # retval = os.getcwd()\n",
    "            os.chdir(\"CLEF-ENG-ML/\")\n",
    "            p = subprocess.Popen('/cm/shared/apps/java/jdk1.8.0_191/bin/java -cp target/lib/*:target/CLEF-ENG-ML-1.0-SNAPSHOT.jar rabflair.flair.myBatchSearch', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "            line = p.stdout.readlines()[0]\n",
    "            line_splitted = line.decode().split()\n",
    "            mAP = float(line_splitted[3])\n",
    "            #retval = p.wait()\n",
    "            os.chdir(\"../\")\n",
    "            if verbose == True:\n",
    "                print(\"percentage of uncovered terms \" + str(sum_percentage_uncovered/length))\n",
    "                #print(\"corpus bleu score is \" + str(corpus_bleu(list_of_references, hypotheses)) + \" and average MAP is \" + str(mAP))\n",
    "                #print(\"BLEU:\\t\" + str(corpus_bleu(list_of_references, hypotheses)))\n",
    "\n",
    "            print(settings[setting] + \"\\t\" + str(opt.k) + \"\\t\" + str(weight) + \"\\t\" + str(mAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#os.chdir(\"../\")\n",
    "#print(os.getcwd())\n",
    "# for line in query_file_desc:\n",
    "#     print(line)\n",
    "# print(query_file_desc)\n",
    "\n",
    "# def create_galago_query(tuple_list):\n",
    "#     st=\"#combine:\"\n",
    "    \n",
    "#     for index, tuple in enumerate(tuple_list):\n",
    "#         st+= str(index) + \"=\" + str(tuple[1])\n",
    "#         st+=\":\"\n",
    "#     st = st.rstrip(\":\") \n",
    "#     st+=\"(\"\n",
    "#     for index, tuple in enumerate(tuple_list):\n",
    "#          st+= str(tuple[0] + \" \")\n",
    "#     st+=\")\"\n",
    "#     print(st)\n",
    "    \n",
    "# import pandas as pd\n",
    "# import operator\n",
    "# for query in queries:\n",
    "#     sum = 0.0\n",
    "#     for token in query.keys():\n",
    "#         sum+=query[token]\n",
    "#     for token in query.keys():\n",
    "#         query[token]/=sum\n",
    "#     sorted_query = sorted(query.items(), key=operator.itemgetter(1))\n",
    "#     print (sorted_query)\n",
    "#     create_galago_query(sorted_query)\n",
    "#     #print(sorted(queries, lambda key = queries[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tagme \n",
    "# import string\n",
    "# print(type(TRG))\n",
    "# TRG.vocab.stoi['balladur']\n",
    "# tagme.GCUBE_TOKEN = \"58f5b784-bf5a-4840-a928-e214134f98e7-843339462\"\n",
    "# regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# def get_entity_strings(query, language):\n",
    "#     annotations = tagme.annotate(query, lang=language)\n",
    "#     st = ''\n",
    "#     for ann in annotations.get_annotations(0.3):\n",
    "#         #print ann.entity_title\n",
    "#         entity = ann.entity_title.lower()\n",
    "#         st+= entity + ' '\n",
    "#     return st\n",
    "# print(get_entity_strings(\"palmares ayrton senna\", 'it'))\n",
    "#     #print(st)\n",
    "# #opt.text = \"german parlamento documenti artista christo\"\n",
    "# #opt.text = \"trova documenti parlano impacchettamento parlamento tedesco berlino opera artista christo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     length+=1\n",
    "#     #print(key)\n",
    "#     opt.text = it_query_dict[key][1] ###\n",
    "#     print(opt.text)\n",
    "#     candidate, queries = translate(opt, model, SRC, TRG)\n",
    "#     #candidate = ' '.join(queries[0].keys()) ###\n",
    "#     #candidate+= ' '.join(queries[1].keys())\n",
    "#     candidate = candidate + \" \" + opt.text ### it_query_dict[key][0] #get_entity_strings(opt.text, \"it\") ###\n",
    "#     candidate = candidate.lower() ###\n",
    "#     #candidate = create_galago_query(queries[0], opt.text) ### weights estimated by neural approach. The code is put below\n",
    "#     reference = eng_query_dict[key][1] ###\n",
    "#     list_of_references.append(reference)\n",
    "#     hypotheses.append(candidate)\n",
    "#     reference_splitted_list = reference.split() \n",
    "#     reference_splitted_list = [stemmer.stem(token) for token in reference_splitted_list]    \n",
    "#     reference_splitted = set(reference_splitted_list)\n",
    "#     candidate_splitted_list = candidate.split() \n",
    "#     candidate_splitted_list = [stemmer.stem(token)  for token in candidate_splitted_list]\n",
    "#     candidate_splitted = set(candidate_splitted_list)\n",
    "#     #reference = ' '.join(reference_splitted)\n",
    "#     #candidate = ' '.join(candidate_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
