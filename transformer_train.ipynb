{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import dill as pickle\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import whoosh, glob, time, pickle\n",
    "import whoosh.fields as wf\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import index\n",
    "import threading\n",
    "from whoosh import filedb\n",
    "from whoosh.filedb.filestore import FileStorage\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from InMemorySearch import *\n",
    "from client import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.141.0.104', '10.141.0.146', '10.141.0.134', '10.141.0.120', '10.141.0.121']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/client.py\", line 38, in execute_on_server\n",
      "    client.connect(host, port).send(query_dict)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/client.py\", line 38, in execute_on_server\n",
      "    client.connect(host, port).send(query_dict)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/jsonsocket.py\", line 81, in connect\n",
      "    self.socket.connect((host, port))\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/jsonsocket.py\", line 81, in connect\n",
      "    self.socket.connect((host, port))\n",
      "Traceback (most recent call last):\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/client.py\", line 38, in execute_on_server\n",
      "    client.connect(host, port).send(query_dict)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/jsonsocket.py\", line 81, in connect\n",
      "    self.socket.connect((host, port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/smsarwar/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/client.py\", line 38, in execute_on_server\n",
      "    client.connect(host, port).send(query_dict)\n",
      "  File \"/mnt/nfs/work1/allan/smsarwar/material/pytorch_transformer/jsonsocket.py\", line 81, in connect\n",
      "    self.socket.connect((host, port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#At first checking our super clients\n",
    "query_list = [\"european crime records\", \"crime records\", \"european crime\", \"european records\", \"crime crimes violent sheriff enforcement re criminals stresak bill strikes\", \"LA times corpus\", \"crime records\"]    \n",
    "super_client = SuperClient()\n",
    "print(super_client.hosts)\n",
    "final_result = super_client.query_expansion_distributed(query_list)\n",
    "print(len(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#If we are working on small dataset\n",
    "small = 0\n",
    "#If we want to activate relevance based training\n",
    "relevance_training = 0\n",
    "    \n",
    "if small==1:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian_small.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english_small.txt')\n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/english_retrieval.txt')\n",
    "\n",
    "else:\n",
    "    parser.add_argument('-src_data', type=str, default='data/italian.txt')\n",
    "    parser.add_argument('-trg_data', type=str, default='data/english.txt')  \n",
    "    parser.add_argument('-trg_data_retrieval', type=str, default='data/LATIMESTEXT2.txt')\n",
    "\n",
    "parser.add_argument('-src_lang', type=str, default='it')\n",
    "parser.add_argument('-trg_lang', type=str, default='en')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=1)\n",
    "parser.add_argument('-d_model', type=int, default=200)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1000)\n",
    "parser.add_argument('-printevery', type=int, default=50)\n",
    "parser.add_argument('-load_vocab', type=str, default='clir_it_en')\n",
    "\n",
    "if relevance_training == 1:\n",
    "    my_file = Path(\"weights/model_weights\")\n",
    "    if my_file.is_file():\n",
    "        parser.add_argument('-load_weights', type=str, default='weights')\n",
    "    else:\n",
    "        parser.add_argument('-load_weights', type=str, default=None)\n",
    "    parser.add_argument('-lr', type=int, default=0.01)\n",
    "else: \n",
    "    my_file = Path(\"weights/model_weights\")\n",
    "    if my_file.is_file():\n",
    "        parser.add_argument('-load_weights', type=str, default='weights')\n",
    "    else:\n",
    "        parser.add_argument('-load_weights', type=str, default=None)         \n",
    "    parser.add_argument('-lr', type=int, default=0.0001)\n",
    "    \n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=5)\n",
    "\n",
    "opt = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "    return text.split()\n",
    "    \n",
    "def create_fields(opt):    \n",
    "    print(\"loading tokenizers...\") \n",
    "    TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=tokenizer)   \n",
    "    SRC = pickle.load(open(f'{opt.load_vocab}/SRC.pkl', 'rb'))\n",
    "    TRG = pickle.load(open(f'{opt.load_vocab}/TRG.pkl', 'rb'))\n",
    "    return(SRC, TRG)\n",
    "\n",
    "#this function will consider both europarl and CLEF\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "    print(\"creating dataset and iterator... \")\n",
    "    \n",
    "    raw_data = {'src' : [line for line in opt.src_data], 'trg': [line for line in opt.trg_data]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "      \n",
    "    os.remove('translate_transformer_temp.csv')   \n",
    "    \n",
    "    if opt.load_vocab is None:\n",
    "        print(\"creating dataset for retrieval corpus... \")\n",
    "        raw_data = {'trg': [line for line in opt.trg_data_retrieval]}\n",
    "        df = pd.DataFrame(raw_data, columns=[\"trg\"])\n",
    "        mask = (df['trg'].str.count(' ') > 1)\n",
    "        df = df.loc[mask]\n",
    "        df.to_csv(\"translate_transformer_retrieval_temp.csv\", index=False)\n",
    "        data_fields = [('trg', TRG)]\n",
    "        train_retrieval = data.TabularDataset('./translate_transformer_retrieval_temp.csv', format='csv', fields=data_fields)\n",
    "        os.remove('translate_transformer_retrieval_temp.csv')    \n",
    "\n",
    "        print(\"building vocabulary for both europarl and retrieval corpus\")\n",
    "\n",
    "        SRC.build_vocab(train)\n",
    "        TRG.build_vocab(train, train_retrieval)\n",
    "\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    return train_iter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading tokenizers...\n",
      "creating dataset and iterator... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained weights...\n",
      "model weights will be saved every 5 minutes and at end of epoch to directory weights/\n"
     ]
    }
   ],
   "source": [
    "opt.device = 0 if opt.no_cuda is False else -1\n",
    "if opt.device == 0:\n",
    "    assert torch.cuda.is_available()\n",
    "print(opt.device)\n",
    "read_data(opt)\n",
    "\n",
    "SRC, TRG = create_fields(opt)\n",
    "opt.train = create_dataset(opt, SRC, TRG)\n",
    "dst = ''\n",
    "#TRG = create_retrieval_vocabulary(opt, TRG)\n",
    "model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "\n",
    "opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "if opt.SGDR == True:\n",
    "    opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)\n",
    "\n",
    "if opt.checkpoint > 0:\n",
    "    print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n",
    "    \n",
    "if opt.load_vocab is None:\n",
    "    pickle.dump(SRC, open(f'{dst}/SRC.pkl', 'wb'))\n",
    "    pickle.dump(TRG, open(f'{dst}/TRG.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_client = SuperClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, opt, SRC, TRG):\n",
    "    torch.cuda.empty_cache()\n",
    "    inmem = WhooshInMemorySearch()\n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    if opt.checkpoint > 0:\n",
    "        cptime = time.time()\n",
    "                    \n",
    "    for epoch in range(opt.epochs):\n",
    "        total_loss = 0\n",
    "        if opt.floyd is False:\n",
    "            print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
    "            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n",
    "        \n",
    "        if opt.checkpoint > 0:\n",
    "            torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                    \n",
    "        for i, batch in enumerate(opt.train):\n",
    "            torch.cuda.empty_cache()\n",
    "            start = time.clock()\n",
    "            src = batch.src.transpose(0,1) # src_size = (187, 4)\n",
    "            trg = batch.trg.transpose(0,1) # trg_size = (187, 8)             \n",
    "            if relevance_training == 1:\n",
    "                trg_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in trg]\n",
    "                trg_strings_rm = super_client.query_expansion_distributed(trg_strings)\n",
    "                \n",
    "                trg_strings_rm_id = torch.LongTensor([[TRG.vocab.stoi[token] for token in sentence] for sentence in trg_strings_rm]).cuda()            \n",
    "                \n",
    "                embed_trg = model.decoder.embed(trg_strings_rm_id)\n",
    "                #old loss\n",
    "                #embed_trg = embed_trg.view(embed_trg.size(0), -1)\n",
    "                #new loss \n",
    "                embed_trg = embed_trg.sum(1)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, opt)            \n",
    "            src_mask = src_mask.cuda()            \n",
    "            trg_mask = trg_mask.cuda()\n",
    "            src = src.cuda() \n",
    "            trg_input = trg_input.cuda()                                  \n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            #the goal is to find the embedding of the predictions \n",
    "            if relevance_training == 1: \n",
    "                out = F.softmax(preds, dim=-1)\n",
    "                probs, ix = out[:, :].data.topk(1)\n",
    "                preds_token_ids = ix.view(ix.size(0), -1)\n",
    "                #new loss function \n",
    "                embed_pred = model.decoder.embed(preds_token_ids)\n",
    "                embed_pred = embed_pred.sum(1)\n",
    "                #print(\"embed preds size \" + str(embed_pred_sum.size()))            \n",
    "\n",
    "                #old loss function\n",
    "#                 pred_strings = [' '.join([TRG.vocab.itos[ind] for ind in ex]) for ex in preds_token_ids]\n",
    "#                 pred_strings_rm = super_client.query_expansion_distributed(pred_strings)                                \n",
    "#                 pred_strings_rm_id = torch.LongTensor([[TRG.vocab.stoi[token] for token in sentence] for sentence in pred_strings_rm]).cuda()            \n",
    "#                 embed_pred = model.decoder.embed(pred_strings_rm_id)\n",
    "#                 embed_pred = embed_pred.view(embed_pred.size(0), -1)            \n",
    "                     \n",
    "            \n",
    "            ys = trg[:, 1:].contiguous().view(-1).cuda()\n",
    "            \n",
    "            opt.optimizer.zero_grad()\n",
    "            \n",
    "            if relevance_training==1:\n",
    "                #loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad).add(\n",
    "                #((embed_trg.cuda() - embed_pred.cuda()) **2).mean())\n",
    "                loss1 = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)              \n",
    "                loss2 = ((embed_trg.cuda() - embed_pred.cuda()) **2).mean()\n",
    "                print(\"batch loss nmt\\t\" + str(loss1.item()) + \"\\tbatch loss relevance\\t\" + str(loss2.item()))\n",
    "                loss1.backward(retain_graph=True)\n",
    "                loss2.backward()               \n",
    "            else:\n",
    "                loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)              \n",
    "                loss.backward()             \n",
    "            \n",
    "            opt.optimizer.step()\n",
    "                \n",
    "            if opt.SGDR == True: \n",
    "                opt.sched.step()\n",
    "            #total_loss += loss1.item() + loss2.item()\n",
    "            total_loss+= loss.item()\n",
    "            \n",
    "            if (i + 1) % opt.printevery == 0:\n",
    "                 p = int(100 * (i + 1) / opt.train_len)\n",
    "                 avg_loss = total_loss/opt.printevery\n",
    "                 if opt.floyd is False:\n",
    "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='\\r')\n",
    "                 else:\n",
    "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n",
    "                 total_loss = 0\n",
    "            \n",
    "            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "                torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                cptime = time.time()\n",
    "                        \n",
    "        print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "25794681m: epoch 1 [####################]  100%  loss = 3.41313\n",
      "epoch 1 complete, loss = 3.413\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "model = model.cuda()\n",
    "train_model(model, opt, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d9befca6cec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{load_vocab}/model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# x = torch.randn(4,6,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print (x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# x = x.view(4, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'{load_vocab}/model_weights')\n",
    "\n",
    "# x = torch.randn(4,6,1)\n",
    "# print (x)\n",
    "# x = x.view(4, -1)\n",
    "# print (x)\n",
    "# # print(x)\n",
    "# # probs, ix = x[:, :].data.topk(1)\n",
    "# # print(probs)\n",
    "# # print(ix)\n",
    "\n",
    "# x = torch.randn(3,2)\n",
    "# print(x)\n",
    "# y = torch.randn(3,2)\n",
    "# print(y)\n",
    "# print (((x - y)**2).mean())\n",
    "# #F.cosine_embedding_loss(x, y, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(SRC.vocab.itos[4880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
